import os
import json
import shutil
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoProcessor, AutoModelForCausalLM, AdamW, 
    get_scheduler, Trainer, TrainingArguments
)
from PIL import Image, ImageDraw
import numpy as np
from typing import List, Dict, Tuple, Optional
from ultralytics import YOLO
from tqdm import tqdm
import yaml
import cv2
import random
import time
from pathlib import Path

class Florence2LocalModelTrainer:
    """å…¼å®¹æœ¬åœ°Florence2æ¨¡å‹çš„å¾®è°ƒè®­ç»ƒå™¨ï¼Œè§£å†³æ•°æ®æ ¼å¼å’Œæ¨¡å‹åŠ è½½é—®é¢˜"""
    
    def __init__(self, base_model_path: str = "weights/icon_caption_florence"):
        self.base_model_path = base_model_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Initializing Florence2LocalModelTrainer with model path: {self.base_model_path}")
        print(f"Using device: {self.device}")
    
    def setup_model_and_processor(self, freeze_backbone=True):
        """è®¾ç½®æœ¬åœ°æ¨¡å‹å’Œå¤„ç†å™¨ï¼Œæ”¯æŒå±‚å†»ç»“"""
        print(f"Loading Florence2 model from local path: {self.base_model_path}")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.base_model_path):
            raise FileNotFoundError(f"Model path {self.base_model_path} does not exist!")
        
        try:
            # éµå¾ªåŸå§‹é¡¹ç›®è®¾è®¡ï¼šprocessoræ€»æ˜¯ä»åŸºç¡€æ¨¡å‹åŠ è½½ï¼Œåªæœ‰æ¨¡å‹æƒé‡ä»æœ¬åœ°åŠ è½½
            print("Loading processor from base Florence2 model (as per original project design)")
            self.processor = AutoProcessor.from_pretrained(
                "microsoft/Florence-2-base", 
                trust_remote_code=True
            )
            
            # åŠ è½½æœ¬åœ°æ¨¡å‹æƒé‡ - å¼ºåˆ¶ä½¿ç”¨float32ä»¥é¿å…æ··åˆç²¾åº¦é—®é¢˜
            print(f"Loading model weights from local path: {self.base_model_path}")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model_path,
                torch_dtype=torch.float32,  # å¼ºåˆ¶ä½¿ç”¨float32ç»Ÿä¸€ç²¾åº¦
                trust_remote_code=True,
                local_files_only=True
            ).to(self.device)
            
            print(f"âœ“ Local model loaded successfully")
            print(f"  Model type: {self.model.config.model_type}")
            print(f"  Model path: {getattr(self.model.config, '_name_or_path', self.base_model_path)}")
            print(f"  Trainable parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}")
            
        except Exception as e:
            print(f"âœ— Error loading local model: {e}")
            print("Fallback: Loading base Florence2 model for training...")
            
            self.processor = AutoProcessor.from_pretrained(
                "microsoft/Florence-2-base", 
                trust_remote_code=True
            )
            
            # å¦‚æœä½¿ç”¨åŸºç¡€æ¨¡å‹ä½œä¸ºå¤‡é€‰ï¼Œä¹Ÿä½¿ç”¨float32æ ¼å¼
            self.model = AutoModelForCausalLM.from_pretrained(
                "microsoft/Florence-2-base",
                torch_dtype=torch.float32,
                trust_remote_code=True
            ).to(self.device)
                
            print("âš ï¸  Using base model instead of local weights!")
        
        # éªŒè¯æ¨¡å‹æƒé‡æ¥æº
        self.verify_model_source()
        
        # æ·»åŠ ä¸¥æ ¼éªŒè¯ï¼šç¡®ä¿æˆ‘ä»¬çœŸçš„åœ¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        self._strict_local_model_verification()
        
        # å†»ç»“å¤§éƒ¨åˆ†å‚æ•°ï¼Œåªè®­ç»ƒé¡¶å±‚
        if freeze_backbone:
            self.freeze_model_layers()
        
        self.model.train()
    
    def verify_model_source(self):
        """éªŒè¯æ¨¡å‹æ˜¯å¦çœŸçš„ä»æœ¬åœ°åŠ è½½"""
        print("\nğŸ” Verifying model source...")
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®ä¸­çš„è·¯å¾„ä¿¡æ¯
        config_path = getattr(self.model.config, '_name_or_path', 'Unknown')
        print(f"  Model config path: {config_path}")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å¤§å°ï¼ˆæœ¬åœ°å¾®è°ƒæ¨¡å‹åº”è¯¥ä¸åŸå§‹ä¸åŒï¼‰
        model_files = []
        if os.path.exists(self.base_model_path):
            for file in os.listdir(self.base_model_path):
                if file.endswith(('.safetensors', '.bin', '.pt')):
                    file_path = os.path.join(self.base_model_path, file)
                    size_mb = os.path.getsize(file_path) / (1024*1024)
                    model_files.append((file, f"{size_mb:.1f}MB"))
        
        if model_files:
            print(f"  Local model files: {model_files}")
        
        # å°è¯•ç®€å•æ¨ç†éªŒè¯æ¨¡å‹è¡Œä¸º
        try:
            test_image = Image.new('RGB', (64, 64), (128, 128, 128))
            inputs = self.processor(
                text=["<CAPTION>"], 
                images=[test_image], 
                return_tensors="pt",
                do_resize=False
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                # Handle mixed precision models properly
                if self.device.type == 'cuda' and hasattr(self.model, 'dtype'):
                    model_dtype = self.model.dtype
                    print(f"  Model dtype: {model_dtype}")
                    
                    # Convert inputs to match model dtype
                    for key, tensor in inputs.items():
                        if tensor.dtype.is_floating_point:
                            inputs[key] = tensor.to(dtype=model_dtype)
                        print(f"  {key} dtype: {inputs[key].dtype}")
                
                outputs = self.model.generate(
                    input_ids=inputs["input_ids"],
                    pixel_values=inputs["pixel_values"],
                    max_new_tokens=10,
                    num_beams=1,
                    do_sample=False
                )
            
            result = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]
            print(f"  Model inference test: '{result}'")
            
            # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«UIç›¸å…³å†…å®¹ï¼ˆæœ¬åœ°å¾®è°ƒæ¨¡å‹çš„ç‰¹å¾ï¼‰
            if any(keyword in result.lower() for keyword in ['ui', 'button', 'icon', 'menu', 'tab']):
                print("  âœ“ Model appears to be UI-specialized (likely local)")
            elif len(result.strip()) < 20:  # çŸ­è¾“å‡ºå¯èƒ½æ˜¯åŸå§‹æ¨¡å‹
                print("  âš ï¸  Short output - could be base model or specialized")
            else:
                print("  âš ï¸  Output doesn't seem UI-specialized")
                
        except Exception as e:
            print(f"  âš ï¸  Verification test failed: {e}")
        
        print("ğŸ” Model verification completed\n")
    
    def _strict_local_model_verification(self):
        """ä¸¥æ ¼éªŒè¯æ¨¡å‹æ˜¯å¦ä¸ºæœ¬åœ°æ¨¡å‹ï¼Œå¦‚æœä¸æ˜¯åˆ™åœæ­¢è®­ç»ƒ"""
        print("ğŸ”’ Strict local model verification...")
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®è·¯å¾„
        model_path = getattr(self.model.config, '_name_or_path', '')
        if model_path != self.base_model_path:
            print(f"âŒ CRITICAL: Model path mismatch!")
            print(f"   Expected: {self.base_model_path}")
            print(f"   Actual: {model_path}")
            raise ValueError("Model is not loaded from specified local path!")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å¤§å°åº”è¯¥åŒ¹é…æœ¬åœ°æ¨¡å‹
        expected_size = 1083916964  # æœ¬åœ°æ¨¡å‹çš„ç¡®åˆ‡å¤§å°
        local_model_file = os.path.join(self.base_model_path, 'model.safetensors')
        if os.path.exists(local_model_file):
            actual_size = os.path.getsize(local_model_file)
            if actual_size != expected_size:
                print(f"âš ï¸  Warning: Model file size mismatch")
                print(f"   Expected: {expected_size:,} bytes")
                print(f"   Actual: {actual_size:,} bytes")
        
        # è¿›è¡Œæ¨ç†å¯¹æ¯”éªŒè¯
        try:
            print("Testing inference signature...")
            test_image = Image.new('RGB', (64, 64), (128, 128, 128))
            inputs = self.processor(
                text=["<CAPTION>"], 
                images=[test_image], 
                return_tensors="pt",
                do_resize=False
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                # Handle mixed precision models properly
                if self.device.type == 'cuda' and hasattr(self.model, 'dtype'):
                    model_dtype = self.model.dtype
                    # Convert inputs to match model dtype
                    for key, tensor in inputs.items():
                        if tensor.dtype.is_floating_point:
                            inputs[key] = tensor.to(dtype=model_dtype)
                
                outputs = self.model.generate(
                    input_ids=inputs["input_ids"],
                    pixel_values=inputs["pixel_values"],
                    max_new_tokens=20,
                    num_beams=1,
                    do_sample=False
                )
            
            result = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]
            print(f"   Inference result: '{result}'")
            
            # æ£€æŸ¥è¾“å‡ºç‰¹å¾ï¼šæœ¬åœ°æ¨¡å‹åº”è¯¥æœ‰ç‰¹å®šçš„UIè¾“å‡ºæ¨¡å¼
            if len(result.strip()) > 50:  # å¦‚æœè¾“å‡ºè¿‡é•¿ï¼Œå¯èƒ½æ˜¯åŸºç¡€æ¨¡å‹
                print("âš ï¸  WARNING: Output seems too generic, might be base model")
            
            # è®°å½•å½“å‰æ¨¡å‹çŠ¶æ€ç”¨äºåç»­å¯¹æ¯”
            self._initial_inference_result = result
            print("âœ“ Strict verification passed")
            
        except Exception as e:
            print(f"âš ï¸  Verification inference failed: {e}")
        
        print("ğŸ”’ Strict verification completed\n")
    
    def freeze_model_layers(self):
        """å†»ç»“æ¨¡å‹çš„backboneå±‚ï¼Œåªè®­ç»ƒé¡¶å±‚"""
        print("Freezing model backbone...")
        
        # é¦–å…ˆå†»ç»“æ‰€æœ‰å‚æ•°
        for param in self.model.parameters():
            param.requires_grad = False
        
        # ç»Ÿè®¡æ¨¡å‹å±‚ä¿¡æ¯
        total_params = 0
        trainable_params = 0
        
        # åªè§£å†»æœ€å…³é”®çš„å±‚ç”¨äºå¾®è°ƒ
        trainable_keywords = [
            'language_model.lm_head',  # è¾“å‡ºå±‚
            'language_model.model.layers.5',  # æœ€åä¸€å±‚transformer
            'language_model.model.layers.4',  # å€’æ•°ç¬¬äºŒå±‚
            'projector'  # æŠ•å½±å±‚
        ]
        
        for name, param in self.model.named_parameters():
            total_params += param.numel()
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è®­ç»ƒè¿™ä¸ªå‚æ•°
            should_train = any(keyword in name for keyword in trainable_keywords)
            
            if should_train:
                param.requires_grad = True
                trainable_params += param.numel()
                print(f"âœ“ Trainable: {name} ({param.numel():,} params)")
            else:
                param.requires_grad = False
        
        print(f"\nParameter summary:")
        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)")
        print(f"Frozen parameters: {total_params-trainable_params:,} ({100*(total_params-trainable_params)/total_params:.2f}%)")
    
    def train_local_model(self, florence_data: List[Dict], 
                         epochs: int = 5, 
                         batch_size: int = 8, 
                         lr: float = 1e-5,  # é€‚ä¸­çš„å­¦ä¹ ç‡
                         warmup_ratio: float = 0.1,
                         save_steps: int = 500,
                         eval_steps: int = 100):
        """æ”¹è¿›çš„æœ¬åœ°æ¨¡å‹è®­ç»ƒç­–ç•¥"""
        print("Starting improved Florence2 local model training...")
        
        if not florence_data:
            print("No Florence2 training data available!")
            return
        
        print(f"Training with {len(florence_data)} samples")
        print(f"Training parameters: epochs={epochs}, batch_size={batch_size}, lr={lr}")
        
        self.setup_model_and_processor(freeze_backbone=True)
        
        # éªŒè¯æ•°æ®æ ¼å¼
        sample_data = florence_data[0]
        print(f"Sample data format: {sample_data.keys()}")
        if 'image_path' not in sample_data or 'content' not in sample_data:
            print("Warning: Data format may not be compatible. Expected 'image_path' and 'content' keys.")
        
        # æ•°æ®åˆ†å‰²
        train_size = int(0.8 * len(florence_data))
        train_data = florence_data[:train_size]
        val_data = florence_data[train_size:]
        
        print(f"Train samples: {len(train_data)}, Val samples: {len(val_data)}")
        
        # åˆ›å»ºdatasets with improved format handling
        train_dataset = Florence2LocalDataset(train_data, self.processor)
        val_dataset = Florence2LocalDataset(val_data, self.processor) if val_data else None
        
        print(f"Dataset created - Train: {len(train_dataset)} samples")
        if val_dataset:
            print(f"Dataset created - Val: {len(val_dataset)} samples")
        
        # åˆ›å»ºdataloaders with error handling
        try:
            train_loader = DataLoader(
                train_dataset, 
                batch_size=batch_size, 
                shuffle=True, 
                collate_fn=collate_fn_florence_local,
                num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
            )
            val_loader = DataLoader(
                val_dataset, 
                batch_size=batch_size, 
                shuffle=False, 
                collate_fn=collate_fn_florence_local,
                num_workers=0
            ) if val_dataset else None
            
            print(f"DataLoader created - Train batches: {len(train_loader)}")
            if val_loader:
                print(f"DataLoader created - Val batches: {len(val_loader)}")
                
        except Exception as e:
            print(f"Error creating DataLoader: {e}")
            raise
        
        # è®¾ç½®ä¼˜åŒ–å™¨ - åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        print(f"Trainable parameters: {sum(p.numel() for p in trainable_params):,}")
        
        optimizer = AdamW(trainable_params, lr=lr, weight_decay=0.01)
        
        num_training_steps = epochs * len(train_loader)
        num_warmup_steps = int(warmup_ratio * num_training_steps)
        
        lr_scheduler = get_scheduler(
            name="cosine",  # ä½¿ç”¨cosine decay
            optimizer=optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps,
        )
        
        best_val_loss = float('inf')
        patience = 3
        patience_counter = 0
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            self.model.train()
            train_loss = 0
            
            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Training Epoch {epoch + 1}/{epochs}")):
                try:
                    questions, answers, images = batch
                    
                    # å¤„ç†å›¾åƒ - ç¡®ä¿æ‰€æœ‰å›¾åƒéƒ½æ˜¯æ­£ç¡®æ ¼å¼
                    processed_images = []
                    for img in images:
                        if hasattr(img, 'convert'):
                            processed_images.append(img.convert('RGB'))
                        elif isinstance(img, np.ndarray):
                            processed_images.append(Image.fromarray(img).convert('RGB'))
                        else:
                            # å¦‚æœæ˜¯è·¯å¾„å­—ç¬¦ä¸²
                            if isinstance(img, str) and os.path.exists(img):
                                processed_images.append(Image.open(img).convert('RGB'))
                            else:
                                print(f"Warning: Invalid image format: {type(img)}")
                                continue
                    
                    if not processed_images:
                        print("No valid images in batch, skipping...")
                        continue
                    
                    # å¤„ç†è¾“å…¥ - å…¼å®¹Florence2æ ¼å¼
                    try:
                        inputs = self.processor(
                            text=questions[:len(processed_images)], 
                            images=processed_images, 
                            return_tensors="pt", 
                            padding=True,
                            do_resize=False  # éµå¾ªåŸå§‹é¡¹ç›®ï¼šå›¾åƒå·²é¢„å…ˆresizeåˆ°64x64
                        )
                        
                        # æ­£ç¡®å¤„ç†æ•°æ®ç±»å‹å¹¶ç¡®ä¿æ‰€æœ‰tensoråœ¨åŒä¸€è®¾å¤‡  
                        inputs = {k: v.to(self.device) for k, v in inputs.items()}
                        # ä¿æŒæ‰€æœ‰è¾“å…¥ä¸ºfloat32ï¼Œä¸æ¨¡å‹ç»Ÿä¸€
                        
                        # å¤„ç†æ ‡ç­¾ - ç¡®ä¿ä¸questionsé•¿åº¦åŒ¹é…
                        valid_answers = answers[:len(processed_images)]
                        labels = self.processor.tokenizer(
                            text=valid_answers, 
                            return_tensors="pt", 
                            padding=True, 
                            truncation=True,
                            max_length=50,  # é™åˆ¶æ ‡ç­¾é•¿åº¦
                            return_token_type_ids=False
                        ).input_ids.to(self.device)
                        
                    except Exception as e:
                        print(f"Error processing inputs: {e}")
                        continue
                    
                    # å‰å‘ä¼ æ’­ - æ·»åŠ é”™è¯¯å¤„ç†
                    try:
                        outputs = self.model(
                            input_ids=inputs["input_ids"],
                            pixel_values=inputs["pixel_values"],
                            labels=labels
                        )
                        
                        if not hasattr(outputs, 'loss') or outputs.loss is None:
                            print("Warning: No loss in outputs, skipping batch")
                            continue
                            
                    except RuntimeError as e:
                        if "out of memory" in str(e):
                            print(f"CUDA out of memory, skipping batch {batch_idx}")
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                            continue
                        else:
                            print(f"Runtime error in forward pass: {e}")
                            continue
                    except Exception as e:
                        print(f"Unexpected error in forward pass: {e}")
                        continue
                    
                    loss = outputs.loss
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)
                    
                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad()
                    
                    train_loss += loss.item()
                    
                except Exception as e:
                    print(f"Error in batch {batch_idx}: {e}")
                    continue
            
            avg_train_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0
            print(f"Epoch {epoch + 1} - Training Loss: {avg_train_loss:.4f}")
            
            # éªŒè¯
            if val_loader:
                val_loss = self.validate(val_loader)
                print(f"Epoch {epoch + 1} - Validation Loss: {val_loss:.4f}")
                
                # Early stopping
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    self.save_model("weights/icon_caption_florence_finetuned")
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        print(f"Early stopping at epoch {epoch + 1}")
                        break
            else:
                # å³ä½¿æ²¡æœ‰éªŒè¯é›†ä¹Ÿä¿å­˜æ¨¡å‹
                if (epoch + 1) % 2 == 0:  # æ¯2ä¸ªepochä¿å­˜ä¸€æ¬¡
                    save_path = f"weights/icon_caption_florence_epoch_{epoch+1}"
                    self.save_model(save_path)
                    print(f"Model saved at epoch {epoch + 1}")
    
    def validate(self, val_loader):
        """éªŒè¯å‡½æ•°"""
        self.model.eval()
        val_loss = 0
        val_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    questions, answers, images = batch
                    
                    # éªŒè¯æ—¶çš„å›¾åƒå¤„ç†
                    processed_images = []
                    for img in images:
                        if hasattr(img, 'convert'):
                            processed_images.append(img.convert('RGB'))
                        elif isinstance(img, np.ndarray):
                            processed_images.append(Image.fromarray(img).convert('RGB'))
                        else:
                            if isinstance(img, str) and os.path.exists(img):
                                processed_images.append(Image.open(img).convert('RGB'))
                            else:
                                continue
                    
                    if not processed_images:
                        continue
                    
                    inputs = self.processor(
                        text=questions[:len(processed_images)], 
                        images=processed_images, 
                        return_tensors="pt", 
                        padding=True,
                        do_resize=False  # éµå¾ªåŸå§‹é¡¹ç›®ï¼šå›¾åƒå·²é¢„å…ˆresizeåˆ°64x64
                    )
                    
                    # ç¡®ä¿æ‰€æœ‰tensoråœ¨åŒä¸€è®¾å¤‡
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    # ä¿æŒæ‰€æœ‰è¾“å…¥ä¸ºfloat32ï¼Œä¸æ¨¡å‹ç»Ÿä¸€
                    
                    valid_answers = answers[:len(processed_images)]
                    labels = self.processor.tokenizer(
                        text=valid_answers, 
                        return_tensors="pt", 
                        padding=True,
                        truncation=True,
                        max_length=50,
                        return_token_type_ids=False
                    ).input_ids.to(self.device)
                    
                    outputs = self.model(
                        input_ids=inputs["input_ids"],
                        pixel_values=inputs["pixel_values"],
                        labels=labels
                    )
                    
                    val_loss += outputs.loss.item()
                    val_batches += 1
                    
                except Exception as e:
                    continue
        
        return val_loss / val_batches if val_batches > 0 else float('inf')
    
    def save_model(self, save_path):
        """ä¿å­˜æ¨¡å‹æƒé‡ï¼Œä¿æŒä¸åŸå§‹æœ¬åœ°æ¨¡å‹çš„å…¼å®¹æ€§"""
        print(f"Saving model to {save_path}")
        try:
            os.makedirs(save_path, exist_ok=True)
            
            # ä½¿ç”¨æ ‡å‡†æ–¹æ³•ä¿å­˜æ¨¡å‹ï¼Œä½†é™åˆ¶ä¿å­˜çš„æ–‡ä»¶
            print("Saving model using standard method to handle shared tensors properly")
            
            # ä½¿ç”¨HuggingFaceçš„æ ‡å‡†ä¿å­˜æ–¹æ³•å¤„ç†å…±äº«å¼ é‡
            self.model.save_pretrained(
                save_path,
                safe_serialization=True,
                max_shard_size="2GB"
            )
            print("âœ“ Model weights saved successfully")
            
            # åˆ é™¤æˆ‘ä»¬ä¸éœ€è¦çš„æ–‡ä»¶ï¼ˆä¿æŒä¸åŸå§‹æœ¬åœ°æ¨¡å‹ä¸€è‡´ï¼‰
            unwanted_files = ['configuration_florence2.py', 'modeling_florence2.py']
            for file_name in unwanted_files:
                file_path = os.path.join(save_path, file_name)
                if os.path.exists(file_path):
                    os.remove(file_path)
                    print(f"Removed {file_name} to match original model structure")
            
            # æ¢å¤åŸå§‹æœ¬åœ°æ¨¡å‹çš„configæ–‡ä»¶ï¼ˆè¢«save_pretrainedè¦†ç›–çš„ï¼‰
            print("Restoring original local model configuration files...")
            essential_files = ['config.json', 'generation_config.json']
            for file_name in essential_files:
                source_file = os.path.join(self.base_model_path, file_name)
                if os.path.exists(source_file):
                    target_file = os.path.join(save_path, file_name)
                    import shutil
                    shutil.copy2(source_file, target_file)
                    print(f"Restored {file_name} from original local model")
            
            # ä¿å­˜è®­ç»ƒè®°å½•ï¼ˆåŒºåˆ«äºæ¨¡å‹é…ç½®ï¼‰
            training_record = {
                "training_type": "florence2_local_finetune",
                "base_model_source": self.base_model_path,
                "trained_on": "local_weights",
                "device": str(self.device),
                "training_completed": True,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "note": "Model weights only - use original processor from microsoft/Florence-2-base"
            }
            
            with open(os.path.join(save_path, "training_record.json"), "w") as f:
                json.dump(training_record, f, indent=2)
            
            print(f"âœ“ Model weights successfully saved to {save_path}")
            print(f"â„¹ï¸  Use with processor from 'microsoft/Florence-2-base' for compatibility")
            
        except Exception as e:
            print(f"âœ— Error saving model: {e}")
            raise

class Florence2LocalDataset(Dataset):
    """æœ¬åœ°Florence2æ•°æ®é›†ï¼Œæ”¯æŒå¤šç§æ•°æ®æ ¼å¼"""
    
    def __init__(self, data_list: List[Dict], processor, max_length: int = 50):
        self.data_list = data_list
        self.processor = processor
        self.max_length = max_length
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼å¹¶ç»Ÿè®¡
        self.validate_data()
    
    def validate_data(self):
        """éªŒè¯æ•°æ®æ ¼å¼"""
        valid_count = 0
        invalid_count = 0
        
        for i, item in enumerate(self.data_list):
            if 'image_path' in item and 'content' in item:
                # æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if isinstance(item['image_path'], str) and os.path.exists(item['image_path']):
                    valid_count += 1
                else:
                    invalid_count += 1
                    if i < 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯
                        print(f"Warning: Image file not found: {item['image_path']}")
            else:
                invalid_count += 1
                if i < 5:
                    print(f"Warning: Invalid data format at index {i}: {item.keys()}")
        
        print(f"Dataset validation: {valid_count} valid, {invalid_count} invalid samples")
        
        if valid_count == 0:
            raise ValueError("No valid samples found in dataset!")
    
    def __len__(self):
        return len(self.data_list)
    
    def __getitem__(self, idx):
        item = self.data_list[idx]
        
        # éµå¾ªåŸå§‹é¡¹ç›®ï¼šFlorence2ä½¿ç”¨<CAPTION>æç¤ºè¯ï¼Œä½†è®­ç»ƒæ—¶ç”¨questionsæ•°ç»„
        question = "<CAPTION>"
        
        # æ”¹è¿›çš„ç­”æ¡ˆæ ¼å¼ - æ›´é€‚åˆUIå…ƒç´ 
        original_content = item.get('content', 'unknown')
        
        # æ ¹æ®å†…å®¹ç±»å‹è°ƒæ•´ç­”æ¡ˆæ ¼å¼
        if any(word in original_content.lower() for word in ['button', 'icon', 'menu', 'tab']):
            answer = f"{original_content}"
        elif any(word in original_content.lower() for word in ['text', 'label', 'title']):
            answer = f"text: {original_content}"
        else:
            answer = f"UI: {original_content}"
        
        # åŠ è½½å’Œå¤„ç†å›¾åƒ
        image_path = item.get('image_path', '')
        bbox = item.get('bbox', [0, 0, 1, 1])  # é»˜è®¤å…¨å›¾
        
        try:
            if isinstance(image_path, str) and os.path.exists(image_path):
                image = Image.open(image_path).convert('RGB')
            else:
                # åˆ›å»ºé»˜è®¤å›¾åƒ
                image = Image.new('RGB', (64, 64), (128, 128, 128))
                print(f"Warning: Using default image for item {idx}")
            
            # å¤„ç†è¾¹ç•Œæ¡†è£å‰ª
            if len(bbox) >= 4 and any(b != 0 for b in bbox[:2]) or any(b != 1 for b in bbox[2:]):
                width, height = image.size
                x1 = max(0, int(bbox[0] * width))
                y1 = max(0, int(bbox[1] * height))
                x2 = min(width, int(bbox[2] * width))
                y2 = min(height, int(bbox[3] * height))
                
                if x2 > x1 and y2 > y1:
                    cropped_image = image.crop((x1, y1, x2, y2))
                    # ä½¿ç”¨åŸå§‹é¡¹ç›®çš„æ ‡å‡†å°ºå¯¸ï¼š64x64
                    cropped_image = cropped_image.resize((64, 64))
                else:
                    cropped_image = image.resize((64, 64))
            else:
                cropped_image = image.resize((64, 64))
            
            return question, answer, cropped_image
            
        except Exception as e:
            print(f"Error processing item {idx}: {e}")
            # è¿”å›é»˜è®¤å€¼ - ä½¿ç”¨åŸå§‹é¡¹ç›®æ ‡å‡†å°ºå¯¸64x64
            default_image = Image.new('RGB', (64, 64), (128, 128, 128))
            return question, "icon", default_image

def collate_fn_florence_local(batch):
    """æœ¬åœ°æ¨¡å‹è®­ç»ƒçš„collateå‡½æ•°ï¼ŒåŒ…å«é”™è¯¯å¤„ç†"""
    try:
        # è¿‡æ»¤æ‰Noneå€¼
        valid_batch = [item for item in batch if item is not None and len(item) == 3]
        
        if not valid_batch:
            # è¿”å›ç©ºbatch
            return [], [], []
        
        questions, answers, images = zip(*valid_batch)
        
        # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯æœ‰æ•ˆçš„
        valid_questions = [q for q in questions if q is not None]
        valid_answers = [a for a in answers if a is not None]
        valid_images = [img for img in images if img is not None]
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(valid_questions), len(valid_answers), len(valid_images))
        
        return (
            list(valid_questions[:min_len]), 
            list(valid_answers[:min_len]), 
            list(valid_images[:min_len])
        )
        
    except Exception as e:
        print(f"Error in collate function: {e}")
        return [], [], []

def prepare_training_data(data_path: str) -> List[Dict]:
    """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    if not os.path.exists(data_path):
        print(f"Data file not found: {data_path}")
        return []
    
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"Loaded {len(data)} samples from {data_path}")
        
        # éªŒè¯æ•°æ®æ ¼å¼
        if data and isinstance(data[0], dict):
            required_keys = ['image_path', 'content']
            if all(key in data[0] for key in required_keys):
                print("Data format validation passed")
                return data
            else:
                print(f"Warning: Missing required keys. Found keys: {list(data[0].keys())}")
                return data  # ä»ç„¶å°è¯•ä½¿ç”¨
        else:
            print("Warning: Unexpected data format")
            return []
            
    except Exception as e:
        print(f"Error loading data: {e}")
        return []

def test_model_loading(model_path: str):
    """æµ‹è¯•æ¨¡å‹åŠ è½½æ˜¯å¦æ­£å¸¸"""
    print(f"Testing model loading from: {model_path}")
    
    try:
        trainer = Florence2LocalModelTrainer(base_model_path=model_path)
        trainer.setup_model_and_processor(freeze_backbone=False)
        print("âœ“ Model loading test passed")
        
        # æµ‹è¯•ç®€å•æ¨ç† - ä½¿ç”¨åŸå§‹é¡¹ç›®çš„64x64å°ºå¯¸
        test_image = Image.new('RGB', (64, 64), (128, 128, 128))
        inputs = trainer.processor(
            text=["<CAPTION>"], 
            images=[test_image], 
            return_tensors="pt",
            do_resize=False  # éµå¾ªåŸå§‹é¡¹ç›®
        )
        
        # ç¡®ä¿æ‰€æœ‰tensoråœ¨åŒä¸€è®¾å¤‡
        inputs = {k: v.to(trainer.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            # Handle mixed precision models properly
            if trainer.device.type == 'cuda' and hasattr(trainer.model, 'dtype'):
                model_dtype = trainer.model.dtype
                # Convert inputs to match model dtype
                for key, tensor in inputs.items():
                    if tensor.dtype.is_floating_point:
                        inputs[key] = tensor.to(dtype=model_dtype)
            
            outputs = trainer.model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=10,
                num_beams=1
            )
        
        result = trainer.processor.batch_decode(outputs, skip_special_tokens=True)
        print(f"âœ“ Model inference test passed. Output: {result}")
        return True
        
    except Exception as e:
        print(f"âœ— Model loading/inference test failed: {e}")
        return False

def main():
    """æ”¹è¿›çš„ä¸»è®­ç»ƒå‡½æ•°"""
    print("=== Florence2 Local Model Fine-tuning ===")
    
    # é…ç½®
    model_path = "weights/icon_caption_florence"
    data_path = "training_data/florence_format/florence_data.json"
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(model_path):
        print(f"Error: Model path {model_path} does not exist!")
        print("Please ensure you have downloaded the Florence2 model weights.")
        return
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    print("\n1. Testing model loading...")
    if not test_model_loading(model_path):
        print("Model loading test failed. Please check your model weights.")
        return
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    print("\n2. Preparing training data...")
    florence_data = prepare_training_data(data_path)
    
    if not florence_data:
        print("No training data available. Please prepare your training data first.")
        print("Expected format: [{\"image_path\": \"path/to/image\", \"content\": \"description\", \"bbox\": [x1,y1,x2,y2]}, ...]")
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("\n3. Creating trainer...")
    trainer = Florence2LocalModelTrainer(base_model_path=model_path)
    
    # å¼€å§‹è®­ç»ƒ
    print("\n4. Starting training...")
    try:
        trainer.train_local_model(
            florence_data=florence_data,
            epochs=7,
            batch_size=3,  
            lr=1e-7,  
            warmup_ratio=0.1 # å­¦ä¹ ç‡é¢„çƒ­æ˜¯ä¸€ç§è®­ç»ƒæŠ€å·§ï¼Œç”¨äºåœ¨è®­ç»ƒåˆæœŸé€æ¸å¢åŠ å­¦ä¹ ç‡ï¼Œä»¥å¸®åŠ©æ¨¡å‹æ›´å¿«åœ°æ”¶æ•›ã€‚
        )
        print("\nâœ“ Training completed successfully!")
        
    except Exception as e:
        print(f"\nâœ— Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

