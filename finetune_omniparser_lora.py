import os
import json
import shutil
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoProcessor, AutoModelForCausalLM, 
    get_scheduler, Trainer, TrainingArguments
)
from torch.optim import AdamW
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from PIL import Image, ImageDraw
import numpy as np
from typing import List, Dict, Tuple, Optional
from ultralytics import YOLO
from tqdm import tqdm
import yaml
import cv2
import random
import time
import argparse
from pathlib import Path

class Florence2LoRAModelTrainer:
    """
    ä½¿ç”¨LoRAæ–¹æ³•çš„Florence2æ¨¡å‹å¾®è°ƒè®­ç»ƒå™¨
    
    ä¸»è¦ç‰¹æ€§ï¼š
    - åŠ¨æ€æ£€æµ‹æ‰€æœ‰çº¿æ€§å±‚ä½œä¸ºLoRAç›®æ ‡
    - ä¼˜å…ˆä»æœ¬åœ°è·¯å¾„åŠ è½½processorï¼Œå¤±è´¥æ—¶å›é€€åˆ°åŸºç¡€æ¨¡å‹
    - æ¨¡å—åŒ–çš„ä»£ç ç»“æ„ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•
    - å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
    
    ç›¸æ¯”ä¼ ç»Ÿå±‚å†»ç»“æ–¹æ³•çš„ä¼˜åŠ¿ï¼š
    - å†…å­˜ä½¿ç”¨é‡å¤§å¹…å‡å°‘ï¼ˆ99%+å†…å­˜èŠ‚çœï¼‰
    - è®­ç»ƒå‚æ•°æå°‘ï¼ˆ<1%çš„æ¨¡å‹å‚æ•°ï¼‰
    - æ”¯æŒå¤šä»»åŠ¡é€‚é…å™¨åˆ‡æ¢
    - æ›´é€‚åˆå¤§è§„æ¨¡éƒ¨ç½²
    """
    
    def __init__(self, base_model_path: str = "weights/icon_caption_florence", use_bfloat16: bool = False):
        self.base_model_path = base_model_path
        # ä¿®å¤ï¼šProcessoråº”è¯¥ä»æ ‡å‡†åœ¨çº¿æ¨¡å‹åŠ è½½ï¼Œä¸æ˜¯æœ¬åœ°è·¯å¾„
        self.processor_model_path = "microsoft/Florence-2-base"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.use_bfloat16 = use_bfloat16 and torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8
        self.dtype = torch.bfloat16 if self.use_bfloat16 else torch.float32
        
        print(f"Initializing Florence2LoRAModelTrainer with model path: {self.base_model_path}")
        print(f"Using device: {self.device}")
        if self.use_bfloat16:
            print(f"âœ“ Using bfloat16 for improved efficiency (GPU supports Ampere+)")
        else:
            print(f"Using float32 for maximum compatibility")
            if use_bfloat16 and not self.use_bfloat16:
                print(f"âš ï¸  bfloat16 requested but not available (requires Ampere+ GPU)")
    
    def setup_model_and_processor(self, lora_r=16, lora_alpha=32, lora_dropout=0.1):
        """è®¾ç½®æœ¬åœ°æ¨¡å‹å’Œå¤„ç†å™¨ï¼Œåº”ç”¨LoRAå¾®è°ƒ"""
        print(f"Loading Florence2 model from local path: {self.base_model_path}")
        
        # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.base_model_path):
            raise FileNotFoundError(f"Local model path {self.base_model_path} does not exist!")
        
        try:
            # ä¿®å¤1ï¼šå§‹ç»ˆä»æ ‡å‡†ä½ç½®åŠ è½½processorï¼ˆéµå¾ªåŸå§‹é¡¹ç›®è®¾è®¡ï¼‰
            print(f"Loading processor from standard location: {self.processor_model_path}")
            self.processor = AutoProcessor.from_pretrained(
                self.processor_model_path, 
                trust_remote_code=True
            )
            print("âœ“ Processor loaded from standard location")
            
            # ä¿®å¤2ï¼šå§‹ç»ˆä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹æƒé‡
            print(f"Loading model weights from local path: {self.base_model_path}")
            print(f"Using dtype: {self.dtype}")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model_path,
                torch_dtype=self.dtype,
                trust_remote_code=True,
                local_files_only=True
            ).to(self.device)
            
            print(f"âœ“ Local model loaded successfully")
            print(f"  Model type: {self.model.config.model_type}")
            print(f"  Model path: {getattr(self.model.config, '_name_or_path', self.base_model_path)}")
            
        except Exception as e:
            print(f"âœ— Error loading local model: {e}")
            print("âŒ CRITICAL: Local model loading failed!")
            print(f"âŒ Expected model path: {self.base_model_path}")
            print("âŒ Training/merge MUST use local model weights, not falling back to online model")
            print("âŒ Please ensure the local model exists and is accessible")
            # ä¸å†å›é€€åˆ°åœ¨çº¿æ¨¡å‹ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
            raise ValueError(f"Cannot load local model from {self.base_model_path}. Training requires local weights.") from e
        
        # éªŒè¯æ¨¡å‹æƒé‡æ¥æº
        self.verify_model_source()
        
        # åº”ç”¨LoRAé…ç½®
        self.apply_lora_config(lora_r, lora_alpha, lora_dropout)
        
        self.model.train()
    
    def verify_model_source(self):
        """éªŒè¯æ¨¡å‹æ˜¯å¦çœŸçš„ä»æœ¬åœ°åŠ è½½"""
        print("\nğŸ” Verifying model source...")
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®ä¸­çš„è·¯å¾„ä¿¡æ¯
        config_path = getattr(self.model.config, '_name_or_path', 'Unknown')
        print(f"  Model config path: {config_path}")
        
        # CRITICAL: ç¡®ä¿æ¨¡å‹è·¯å¾„åŒ¹é…
        if config_path != self.base_model_path:
            print(f"âŒ CRITICAL ERROR: Model path mismatch!")
            print(f"   Expected: {self.base_model_path}")
            print(f"   Actual: {config_path}")
            raise ValueError("Model is not loaded from the specified local path! This will cause incorrect training/merge results.")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å¤§å° - éªŒè¯æ˜¯æœ¬åœ°1GBæ¨¡å‹è€Œéåœ¨çº¿270MBæ¨¡å‹
        expected_size = 1083916964  # æœ¬åœ°æ¨¡å‹çš„ç¡®åˆ‡å¤§å° (~1GB)
        local_model_file = os.path.join(self.base_model_path, 'model.safetensors')
        if os.path.exists(local_model_file):
            actual_size = os.path.getsize(local_model_file)
            print(f"  Local model size: {actual_size:,} bytes ({actual_size/(1024*1024):.1f}MB)")
            if actual_size == expected_size:
                print("  âœ“ Model size matches expected local model")
            else:
                print(f"  âš ï¸  Warning: Model file size unexpected")
                print(f"     Expected: {expected_size:,} bytes")
                print(f"     Actual: {actual_size:,} bytes")
        
        # æ£€æŸ¥å‚æ•°æ•°é‡ - æœ¬åœ°æ¨¡å‹åº”è¯¥æœ‰ç‰¹å®šçš„å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"  Total model parameters: {total_params:,}")
        
        # ç®€å•æ¨ç†éªŒè¯æ¨¡å‹è¡Œä¸º
        try:
            test_image = Image.new('RGB', (64, 64), (128, 128, 128))
            inputs = self.processor(
                text=["<CAPTION>"], 
                images=[test_image], 
                return_tensors="pt",
                do_resize=False
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                # Handle mixed precision models properly
                if self.device.type == 'cuda' and hasattr(self.model, 'dtype'):
                    model_dtype = self.model.dtype
                    print(f"  Model dtype: {model_dtype}")
                    
                    # Convert inputs to match model dtype
                    for key, tensor in inputs.items():
                        if tensor.dtype.is_floating_point:
                            inputs[key] = tensor.to(dtype=model_dtype)
                
                outputs = self.model.generate(
                    input_ids=inputs["input_ids"],
                    pixel_values=inputs["pixel_values"],
                    max_new_tokens=10,
                    num_beams=1,
                    do_sample=False
                )
            
            result = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]
            print(f"  Model inference test: '{result}'")
            
        except Exception as e:
            print(f"  âš ï¸  Verification test failed: {e}")
        
        print("âœ“ Model source verification completed - using local model\n")
    
    def apply_lora_config(self, lora_r=16, lora_alpha=32, lora_dropout=0.1):
        """åº”ç”¨LoRAé…ç½®åˆ°æ¨¡å‹"""
        print(f"Applying LoRA configuration (r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout})...")
        
        # åŠ¨æ€æ£€æµ‹æ‰€æœ‰çº¿æ€§å±‚ä½œä¸ºLoRAç›®æ ‡æ¨¡å—
        all_linear_layers = set()
        
        for name, module in self.model.named_modules():
            # æ£€æŸ¥å½“å‰æ¨¡å—æ˜¯å¦æ˜¯ torch.nn.Linear ç±»çš„å®ä¾‹
            # è¿™æ˜¯ LoRA æœ€ä¸»è¦çš„åº”ç”¨å¯¹è±¡
            if isinstance(module, torch.nn.Linear):
                # å¦‚æœæ˜¯çº¿æ€§å±‚ï¼Œå°±å°†å…¶åç§°æ·»åŠ åˆ° set ä¸­
                all_linear_layers.add(name)
        
        print(f"Found {len(all_linear_layers)} linear layers in the model")
        
        # æ˜¾ç¤ºæ‰€æœ‰çº¿æ€§å±‚ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print("All linear layers found:")
        for layer in sorted(list(all_linear_layers))[:20]:  # æ˜¾ç¤ºå‰20ä¸ª
            print(f"  - {layer}")
        if len(all_linear_layers) > 20:
            print(f"  ... and {len(all_linear_layers) - 20} more layers")

        # ç­›é€‰æ³¨æ„åŠ›å±‚å’Œå…³é”®çº¿æ€§å±‚ä½œä¸ºç›®æ ‡æ¨¡å—
        target_modules = [
            name for name in all_linear_layers 
            if "q_proj" in name 
            or "v_proj" in name 
            or "qkv" in name
        ]
        
        print(f"Selected {len(target_modules)} target modules for LoRA:")
        for module in sorted(target_modules):
            print(f"  - {module}")
        
        if not target_modules:
            print("âš ï¸  Warning: No suitable target modules found. Using fallback modules...")
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†çš„æ³¨æ„åŠ›å±‚ï¼Œä½¿ç”¨æ‰€æœ‰çº¿æ€§å±‚ä¸­çš„ä¸€éƒ¨åˆ†
            target_modules = [name for name in all_linear_layers if "linear" in name.lower()][:10]
            if not target_modules:
                # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
                target_modules = list(all_linear_layers)[:5]
            print(f"Fallback target modules: {target_modules}")
        
        # LoRAé…ç½® - ä½¿ç”¨åŠ¨æ€æ£€æµ‹çš„ç›®æ ‡æ¨¡å—
        lora_config = LoraConfig(
            r=lora_r,                    # ç§©å‚æ•°
            lora_alpha=lora_alpha,       # ç¼©æ”¾å› å­
            target_modules=target_modules,  # ä½¿ç”¨åŠ¨æ€æ£€æµ‹çš„æ³¨æ„åŠ›å±‚
            lora_dropout=lora_dropout,
            bias="none",                 # ä¸è®­ç»ƒbias, 
            task_type="CAUSAL_LM"        # å› æœè¯­è¨€æ¨¡å‹
        )
        
        # åº”ç”¨LoRAåˆ°æ¨¡å‹
        try:
            self.model = get_peft_model(self.model, lora_config)
            
            # ç»Ÿè®¡å‚æ•°ä¿¡æ¯
            total_params = 0
            trainable_params = 0
            
            for name, param in self.model.named_parameters():
                total_params += param.numel()
                if param.requires_grad:
                    trainable_params += param.numel()
                    if trainable_params <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªå¯è®­ç»ƒå‚æ•°
                        print(f"âœ“ Trainable: {name} ({param.numel():,} params)")
            
            print(f"\nLoRA Parameter summary:")
            print(f"Total parameters: {total_params:,}")
            print(f"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)")
            print(f"Frozen parameters: {total_params-trainable_params:,} ({100*(total_params-trainable_params)/total_params:.2f}%)")
            
            # æ˜¾ç¤ºLoRAç‰¹æœ‰ä¿¡æ¯
            print(f"LoRA rank (r): {lora_r}")
            print(f"LoRA alpha: {lora_alpha}")
            print(f"LoRA dropout: {lora_dropout}")
            print(f"Estimated memory savings: ~{100*(total_params-trainable_params)/total_params:.1f}%")
            
        except Exception as e:
            print(f"âœ— Error applying LoRA configuration: {e}")
            print("This might be due to module name mismatch or incompatible model structure.")
            print(f"Available linear layers: {sorted(list(all_linear_layers))[:10]}...")
            raise
    
    def _prepare_dataloaders(self, florence_data: List[Dict], batch_size: int):
        """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨"""
        # éªŒè¯æ•°æ®æ ¼å¼
        sample_data = florence_data[0]
        print(f"Sample data format: {sample_data.keys()}")
        if 'image_path' not in sample_data or 'content' not in sample_data:
            print("Warning: Data format may not be compatible. Expected 'image_path' and 'content' keys.")
        
        # æ•°æ®åˆ†å‰²
        train_size = int(0.8 * len(florence_data))
        train_data = florence_data[:train_size]
        val_data = florence_data[train_size:]
        
        print(f"Train samples: {len(train_data)}, Val samples: {len(val_data)}")
        
        # åˆ›å»ºdatasets
        train_dataset = Florence2LoRADataset(train_data, self.processor)
        val_dataset = Florence2LoRADataset(val_data, self.processor) if val_data else None
        
        print(f"Dataset created - Train: {len(train_dataset)} samples")
        if val_dataset:
            print(f"Dataset created - Val: {len(val_dataset)} samples")
        
        # åˆ›å»ºdataloaders
        try:
            train_loader = DataLoader(
                train_dataset, 
                batch_size=batch_size, 
                shuffle=True, 
                collate_fn=collate_fn_florence_lora,
                num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
            )
            val_loader = DataLoader(
                val_dataset, 
                batch_size=batch_size, 
                shuffle=False, 
                collate_fn=collate_fn_florence_lora,
                num_workers=0
            ) if val_dataset else None
            
            print(f"DataLoader created - Train batches: {len(train_loader)}")
            if val_loader:
                print(f"DataLoader created - Val batches: {len(val_loader)}")
            
            return train_loader, val_loader
                
        except Exception as e:
            print(f"Error creating DataLoader: {e}")
            raise
    
    def _create_optimizer_and_scheduler(self, lr: float, warmup_ratio: float, train_loader: DataLoader, epochs: int):
        """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # è®¾ç½®ä¼˜åŒ–å™¨ - åªä¼˜åŒ–LoRAå‚æ•°
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        print(f"LoRA trainable parameters: {sum(p.numel() for p in trainable_params):,}")
        
        # LoRAé€šå¸¸å¯ä»¥ä½¿ç”¨ç¨é«˜çš„å­¦ä¹ ç‡
        optimizer = AdamW(trainable_params, lr=lr, weight_decay=0.01)
        
        num_training_steps = epochs * len(train_loader)
        num_warmup_steps = int(warmup_ratio * num_training_steps)
        
        lr_scheduler = get_scheduler(
            name="cosine",  # ä½¿ç”¨cosine decay
            optimizer=optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps,
        )
        
        return optimizer, lr_scheduler, trainable_params
    
    def train_lora_model(self, florence_data: List[Dict], 
                        epochs: int = 5, 
                        batch_size: int = 8, 
                        lr: float = 1e-4,  # LoRAé€šå¸¸å¯ä»¥ä½¿ç”¨ç¨é«˜çš„å­¦ä¹ ç‡
                        warmup_ratio: float = 0.1,
                        # LoRAé…ç½®å‚æ•°
                        lora_r: int = 16,
                        lora_alpha: int = 32,
                        lora_dropout: float = 0.1):
        """LoRAæ¨¡å‹è®­ç»ƒç­–ç•¥"""
        print("Starting LoRA-based Florence2 model training...")
        
        if not florence_data:
            print("No Florence2 training data available!")
            return
        
        print(f"Training with {len(florence_data)} samples")
        print(f"Training parameters: epochs={epochs}, batch_size={batch_size}, lr={lr}")
        print(f"LoRA parameters: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}")
        
        self.setup_model_and_processor(
            lora_r=lora_r, 
            lora_alpha=lora_alpha, 
            lora_dropout=lora_dropout
        )
        
        # å‡†å¤‡æ•°æ®åŠ è½½å™¨
        train_loader, val_loader = self._prepare_dataloaders(florence_data, batch_size)
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer, lr_scheduler, trainable_params = self._create_optimizer_and_scheduler(lr, warmup_ratio, train_loader, epochs)
        
        best_val_loss = float('inf')
        patience = 3
        patience_counter = 0
        
        # è®­ç»ƒå¾ªç¯ - ä¸åŸå§‹è®­ç»ƒå™¨ç›¸åŒçš„é€»è¾‘
        for epoch in range(epochs):
            self.model.train()
            train_loss = 0
            
            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"LoRA Training Epoch {epoch + 1}/{epochs}")):
                try:
                    questions, answers, images = batch
                    
                    # å¤„ç†å›¾åƒ - ç¡®ä¿æ‰€æœ‰å›¾åƒéƒ½æ˜¯æ­£ç¡®æ ¼å¼
                    processed_images = []
                    for img in images:
                        if hasattr(img, 'convert'):
                            processed_images.append(img.convert('RGB'))
                        elif isinstance(img, np.ndarray):
                            processed_images.append(Image.fromarray(img).convert('RGB'))
                        else:
                            # å¦‚æœæ˜¯è·¯å¾„å­—ç¬¦ä¸²
                            if isinstance(img, str) and os.path.exists(img):
                                processed_images.append(Image.open(img).convert('RGB'))
                            else:
                                print(f"Warning: Invalid image format: {type(img)}")
                                continue
                    
                    if not processed_images:
                        print("No valid images in batch, skipping...")
                        continue
                    
                    # å¤„ç†è¾“å…¥ - å…¼å®¹Florence2æ ¼å¼
                    try:
                        inputs = self.processor(
                            text=questions[:len(processed_images)], 
                            images=processed_images, 
                            return_tensors="pt", 
                            padding=True,
                            do_resize=False  # éµå¾ªåŸå§‹é¡¹ç›®ï¼šå›¾åƒå·²é¢„å…ˆresizeåˆ°64x64
                        )
                        
                        # æ­£ç¡®å¤„ç†æ•°æ®ç±»å‹å¹¶ç¡®ä¿æ‰€æœ‰tensoråœ¨åŒä¸€è®¾å¤‡  
                        inputs = {k: v.to(self.device) for k, v in inputs.items()}
                        
                        # å¤„ç†æ ‡ç­¾ - ç¡®ä¿ä¸questionsé•¿åº¦åŒ¹é…
                        valid_answers = answers[:len(processed_images)]
                        labels = self.processor.tokenizer(
                            text=valid_answers, 
                            return_tensors="pt", 
                            padding=True, 
                            truncation=True,
                            max_length=50,  # é™åˆ¶æ ‡ç­¾é•¿åº¦
                            return_token_type_ids=False
                        ).input_ids.to(self.device)
                        
                    except Exception as e:
                        print(f"Error processing inputs: {e}")
                        continue
                    
                    # å‰å‘ä¼ æ’­ - æ·»åŠ é”™è¯¯å¤„ç†
                    try:
                        outputs = self.model(
                            input_ids=inputs["input_ids"],
                            pixel_values=inputs["pixel_values"],
                            labels=labels
                        )
                        
                        if not hasattr(outputs, 'loss') or outputs.loss is None:
                            print("Warning: No loss in outputs, skipping batch")
                            continue
                            
                    except RuntimeError as e:
                        if "out of memory" in str(e):
                            print(f"CUDA out of memory, skipping batch {batch_idx}")
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                            continue
                        else:
                            print(f"Runtime error in forward pass: {e}")
                            continue
                    except Exception as e:
                        print(f"Unexpected error in forward pass: {e}")
                        continue
                    
                    loss = outputs.loss
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)
                    
                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad()
                    
                    train_loss += loss.item()
                    
                except Exception as e:
                    print(f"Error in batch {batch_idx}: {e}")
                    continue
            
            avg_train_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0
            print(f"Epoch {epoch + 1} - LoRA Training Loss: {avg_train_loss:.4f}")
            
            # éªŒè¯
            if val_loader:
                val_loss = self.validate(val_loader)
                print(f"Epoch {epoch + 1} - Validation Loss: {val_loss:.4f}")
                
                # Early stopping
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    self.save_lora_model("weights/icon_caption_florence_lora_finetuned")
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        print(f"Early stopping at epoch {epoch + 1}")
                        break
            else:
                # å³ä½¿æ²¡æœ‰éªŒè¯é›†ä¹Ÿä¿å­˜æ¨¡å‹
                if (epoch + 1) % 2 == 0:  # æ¯2ä¸ªepochä¿å­˜ä¸€æ¬¡
                    save_path = f"weights/icon_caption_florence_lora_epoch_{epoch+1}"
                    self.save_lora_model(save_path)
                    print(f"LoRA model saved at epoch {epoch + 1}")
    
    def validate(self, val_loader):
        """éªŒè¯å‡½æ•° - ä¸åŸå§‹è®­ç»ƒå™¨ç›¸åŒ"""
        self.model.eval()
        val_loss = 0
        val_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    questions, answers, images = batch
                    
                    # éªŒè¯æ—¶çš„å›¾åƒå¤„ç†
                    processed_images = []
                    for img in images:
                        if hasattr(img, 'convert'):
                            processed_images.append(img.convert('RGB'))
                        elif isinstance(img, np.ndarray):
                            processed_images.append(Image.fromarray(img).convert('RGB'))
                        else:
                            if isinstance(img, str) and os.path.exists(img):
                                processed_images.append(Image.open(img).convert('RGB'))
                            else:
                                continue
                    
                    if not processed_images:
                        continue
                    
                    inputs = self.processor(
                        text=questions[:len(processed_images)], 
                        images=processed_images, 
                        return_tensors="pt", 
                        padding=True,
                        do_resize=False  # éµå¾ªåŸå§‹é¡¹ç›®ï¼šå›¾åƒå·²é¢„å…ˆresizeåˆ°64x64
                    )
                    
                    # ç¡®ä¿æ‰€æœ‰tensoråœ¨åŒä¸€è®¾å¤‡
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    
                    valid_answers = answers[:len(processed_images)]
                    labels = self.processor.tokenizer(
                        text=valid_answers, 
                        return_tensors="pt", 
                        padding=True,
                        truncation=True,
                        max_length=50,
                        return_token_type_ids=False
                    ).input_ids.to(self.device)
                    
                    outputs = self.model(
                        input_ids=inputs["input_ids"],
                        pixel_values=inputs["pixel_values"],
                        labels=labels
                    )
                    
                    val_loss += outputs.loss.item()
                    val_batches += 1
                    
                except Exception as e:
                    continue
        
        return val_loss / val_batches if val_batches > 0 else float('inf')
    
    def save_lora_model(self, save_path):
        """ä¿å­˜LoRAé€‚é…å™¨æƒé‡"""
        print(f"Saving LoRA model to {save_path}")
        try:
            os.makedirs(save_path, exist_ok=True)
            
            # ä¿å­˜LoRAé€‚é…å™¨
            print("Saving LoRA adapter weights...")
            self.model.save_pretrained(
                save_path,
                safe_serialization=True
            )
            print("âœ“ LoRA adapter weights saved successfully")
            
            # ä¿å­˜è®­ç»ƒè®°å½•
            training_record = {
                "training_type": "florence2_lora_finetune",
                "base_model_source": self.base_model_path,
                "training_method": "LoRA",
                "lora_config": {
                    "r": getattr(self.model.peft_config['default'], 'r', 'unknown'),
                    "lora_alpha": getattr(self.model.peft_config['default'], 'lora_alpha', 'unknown'),
                    "lora_dropout": getattr(self.model.peft_config['default'], 'lora_dropout', 'unknown'),
                    "target_modules": list(getattr(self.model.peft_config['default'], 'target_modules', []))
                },
                "device": str(self.device),
                "training_completed": True,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "note": "LoRA adapter only - use with base model from local weights"
            }
            
            with open(os.path.join(save_path, "lora_training_record.json"), "w") as f:
                json.dump(training_record, f, indent=2)
            
            print(f"âœ“ LoRA model successfully saved to {save_path}")
            print(f"â„¹ï¸  Use with base model from '{self.base_model_path}' for inference")
            
        except Exception as e:
            print(f"âœ— Error saving LoRA model: {e}")
            raise

    def merge_and_save_model(self, output_path: str, save_quantized: bool = False):
        """
        åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹å¹¶ä¿å­˜ä¸ºæ–°çš„å®Œæ•´æ¨¡å‹
        Args:
            output_path: ä¿å­˜åˆå¹¶åæ¨¡å‹çš„è·¯å¾„
        æ³¨æ„ï¼šåªä¿å­˜æ¨¡å‹æƒé‡ï¼Œä¸ä¿å­˜processorï¼ˆprocessorä¸è®­ç»ƒæ— å…³ï¼‰
        """
        try:
            print(f"\nğŸ”„ Merging LoRA adapter with base model...")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_path, exist_ok=True)
            
            # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
            merged_model = self.model.merge_and_unload()
            
            # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹æƒé‡
            print(f"ğŸ’¾ Saving merged model weights to {output_path}...")
            merged_model.save_pretrained(output_path)
            
            # å¤åˆ¶åŸºç¡€æ¨¡å‹çš„é…ç½®æ–‡ä»¶ï¼ˆç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„config.jsonï¼‰
            print(f"ğŸ“‹ Copying base model config files...")
            base_config_files = ['config.json', 'generation_config.json']
            for config_file in base_config_files:
                src_path = os.path.join(self.base_model_path, config_file)
                dst_path = os.path.join(output_path, config_file)
                if os.path.exists(src_path):
                    shutil.copy2(src_path, dst_path)
                    print(f"âœ“ Copied {config_file} from base model")
                else:
                    print(f"âš ï¸  {config_file} not found in base model path")
            
            # ä¿å­˜åˆå¹¶ä¿¡æ¯
            merge_record = {
                "model_type": "florence2_merged",
                "base_model_source": self.base_model_path,
                "training_method": "LoRA_merged",
                "merge_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "note": "Merged model with base model configs - ready for direct use"
            }
            
            with open(os.path.join(output_path, "merge_info.json"), "w") as f:
                json.dump(merge_record, f, indent=2)
            
            print(f"âœ“ Model successfully merged and saved to {output_path}")
            print(f"â„¹ï¸  Merged model uses base model configs and can be used directly")
            print(f"â„¹ï¸  Usage: AutoModelForCausalLM.from_pretrained('{output_path}')")
            
            if save_quantized:
                from transformers import BitsAndBytesConfig, AutoProcessor, AutoModelForCausalLM 
                quantization_config_bit = BitsAndBytesConfig(load_in_8bit=True, bnb_8bit_compute_dtype=torch.bfloat16, bnb_8bit_use_double_quant=True)
                model = AutoModelForCausalLM.from_pretrained(output_path, quantization_config=quantization_config_bit, torch_dtype=torch.float32, trust_remote_code=True) # in new version, it automatically select device
                model.save_pretrained("weights/icon_caption_florence_8bit_lora_finetuned")

            return True
            
        except Exception as e:
            print(f"âœ— Error merging model: {e}")
            return False

class Florence2LoRADataset(Dataset):
    """LoRAè®­ç»ƒæ•°æ®é›† - å¤ç”¨åŸå§‹æ•°æ®é›†é€»è¾‘"""
    
    def __init__(self, data_list: List[Dict], processor, max_length: int = 50):
        self.data_list = data_list
        self.processor = processor
        self.max_length = max_length
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼å¹¶ç»Ÿè®¡
        self.validate_data()
    
    def validate_data(self):
        """éªŒè¯æ•°æ®æ ¼å¼"""
        valid_count = 0
        invalid_count = 0
        
        for i, item in enumerate(self.data_list):
            if 'image_path' in item and 'content' in item:
                # æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if isinstance(item['image_path'], str) and os.path.exists(item['image_path']):
                    valid_count += 1
                else:
                    invalid_count += 1
                    if i < 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯
                        print(f"Warning: Image file not found: {item['image_path']}")
            else:
                invalid_count += 1
                if i < 5:
                    print(f"Warning: Invalid data format at index {i}: {item.keys()}")
        
        print(f"LoRA Dataset validation: {valid_count} valid, {invalid_count} invalid samples")
        
        if valid_count == 0:
            raise ValueError("No valid samples found in dataset!")
    
    def __len__(self):
        return len(self.data_list)
    
    def __getitem__(self, idx):
        """ä¸åŸå§‹æ•°æ®é›†ç›¸åŒçš„æ•°æ®å¤„ç†é€»è¾‘"""
        item = self.data_list[idx]
        
        # éµå¾ªåŸå§‹é¡¹ç›®ï¼šFlorence2ä½¿ç”¨<CAPTION>æç¤ºè¯
        question = "<CAPTION>"
        
        # æ”¹è¿›çš„ç­”æ¡ˆæ ¼å¼ - æ›´é€‚åˆUIå…ƒç´ 
        original_content = item.get('content', 'unknown')
        answer = f"{original_content}"
        
        # åŠ è½½å’Œå¤„ç†å›¾åƒ
        image_path = item.get('image_path', '')
        bbox = item.get('bbox', [0, 0, 1, 1])  # é»˜è®¤å…¨å›¾
        
        try:
            if isinstance(image_path, str) and os.path.exists(image_path):
                image = Image.open(image_path).convert('RGB')
            else:
                # åˆ›å»ºé»˜è®¤å›¾åƒ
                image = Image.new('RGB', (64, 64), (128, 128, 128))
                print(f"Warning: Using default image for item {idx}")
            
            # å¤„ç†è¾¹ç•Œæ¡†è£å‰ª
            if len(bbox) >= 4 and any(b != 0 for b in bbox[:2]) or any(b != 1 for b in bbox[2:]):
                width, height = image.size
                x1 = max(0, int(bbox[0] * width))
                y1 = max(0, int(bbox[1] * height))
                x2 = min(width, int(bbox[2] * width))
                y2 = min(height, int(bbox[3] * height))
                
                if x2 > x1 and y2 > y1:
                    cropped_image = image.crop((x1, y1, x2, y2))
                    # ä½¿ç”¨åŸå§‹é¡¹ç›®çš„æ ‡å‡†å°ºå¯¸ï¼š64x64
                    cropped_image = cropped_image.resize((64, 64))
                else:
                    cropped_image = image.resize((64, 64))
            else:
                cropped_image = image.resize((64, 64))
            
            return question, answer, cropped_image
            
        except Exception as e:
            print(f"Error processing item {idx}: {e}")
            # è¿”å›é»˜è®¤å€¼ - ä½¿ç”¨åŸå§‹é¡¹ç›®æ ‡å‡†å°ºå¯¸64x64
            default_image = Image.new('RGB', (64, 64), (128, 128, 128))
            return question, "icon", default_image

def collate_fn_florence_lora(batch):
    """LoRAè®­ç»ƒçš„collateå‡½æ•° - å¤ç”¨åŸå§‹é€»è¾‘"""
    try:
        # è¿‡æ»¤æ‰Noneå€¼
        valid_batch = [item for item in batch if item is not None and len(item) == 3]
        
        if not valid_batch:
            # è¿”å›ç©ºbatch
            return [], [], []
        
        questions, answers, images = zip(*valid_batch)
        
        # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯æœ‰æ•ˆçš„
        valid_questions = [q for q in questions if q is not None]
        valid_answers = [a for a in answers if a is not None]
        valid_images = [img for img in images if img is not None]
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(valid_questions), len(valid_answers), len(valid_images))
        
        return (
            list(valid_questions[:min_len]), 
            list(valid_answers[:min_len]), 
            list(valid_images[:min_len])
        )
        
    except Exception as e:
        print(f"Error in LoRA collate function: {e}")
        return [], [], []

def prepare_training_data(data_path: str) -> List[Dict]:
    """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    if not os.path.exists(data_path):
        print(f"Data file not found: {data_path}")
        return []
    
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"Loaded {len(data)} samples from {data_path}")
        
        # éªŒè¯æ•°æ®æ ¼å¼
        if data and isinstance(data[0], dict):
            required_keys = ['image_path', 'content']
            if all(key in data[0] for key in required_keys):
                print("Data format validation passed")
                return data
            else:
                print(f"Warning: Missing required keys. Found keys: {list(data[0].keys())}")
                return data  # ä»ç„¶å°è¯•ä½¿ç”¨
        else:
            print("Warning: Unexpected data format")
            return []
            
    except Exception as e:
        print(f"Error loading data: {e}")
        return []

def test_lora_model_loading(model_path: str):
    """æµ‹è¯•LoRAæ¨¡å‹åŠ è½½æ˜¯å¦æ­£å¸¸"""
    print(f"Testing LoRA model loading from: {model_path}")
    
    try:
        trainer = Florence2LoRAModelTrainer(base_model_path=model_path)
        trainer.setup_model_and_processor()
        print("âœ“ LoRA model loading test passed")
        
        # æµ‹è¯•ç®€å•æ¨ç†
        test_image = Image.new('RGB', (64, 64), (128, 128, 128))
        inputs = trainer.processor(
            text=["<CAPTION>"], 
            images=[test_image], 
            return_tensors="pt",
            do_resize=False
        )
        
        # ç¡®ä¿æ‰€æœ‰tensoråœ¨åŒä¸€è®¾å¤‡
        inputs = {k: v.to(trainer.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = trainer.model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=10,
                num_beams=1
            )
        
        result = trainer.processor.batch_decode(outputs, skip_special_tokens=True)
        print(f"âœ“ LoRA model inference test passed. Output: {result}")
        return True
        
    except Exception as e:
        print(f"âœ— LoRA model loading/inference test failed: {e}")
        return False

def main():
    """
    LoRAå¾®è°ƒä¸»å‡½æ•°
    
    ä½¿ç”¨æ”¹è¿›çš„LoRAå¾®è°ƒæ–¹æ³•ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š
    1. åŠ¨æ€æ£€æµ‹ç›®æ ‡æ¨¡å— - è‡ªåŠ¨è¯†åˆ«æ‰€æœ‰æ³¨æ„åŠ›å±‚
    2. å¥å£®çš„æ¨¡å‹åŠ è½½ - ä¼˜å…ˆæœ¬åœ°processorï¼Œæ™ºèƒ½å›é€€
    3. æ¨¡å—åŒ–è®¾è®¡ - æ¸…æ™°çš„ä»£ç ç»“æ„ï¼Œæ˜“äºç»´æŠ¤
    4. å¯é€‰bfloat16ä¼˜åŒ– - æ”¯æŒAmpere+æ¶æ„GPUçš„å†…å­˜ä¼˜åŒ–
    
    å¯¹æ¯”ä¸¤ç§å¾®è°ƒæ–¹æ³•çš„ç»“æœï¼š
    - å±‚å†»ç»“æ¨¡å‹: weights/icon_caption_florence_finetuned
    - LoRAæ¨¡å‹: weights/icon_caption_florence_lora_finetuned
    """
    
    print("=== Florence2 LoRA Model Fine-tuning ===")
    
    # é…ç½®
    model_path = "weights/icon_caption_florence"
    data_path = "training_data/florence_format/florence_data.json"
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import peft
        print(f"âœ“ PEFT library version: {peft.__version__}")
    except ImportError:
        print("âœ— PEFT library not found. Please install with: pip install peft")
        return
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(model_path):
        print(f"Error: Model path {model_path} does not exist!")
        print("Please ensure you have downloaded the Florence2 model weights.")
        return
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    print("\n1. Testing LoRA model loading...")
    if not test_lora_model_loading(model_path):
        print("LoRA model loading test failed. Please check your model weights.")
        return
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    print("\n2. Preparing training data...")
    florence_data = prepare_training_data(data_path)
    
    if not florence_data:
        print("No training data available. Please prepare your training data first.")
        print("Expected format: [{\"image_path\": \"path/to/image\", \"content\": \"description\", \"bbox\": [x1,y1,x2,y2]}, ...]")
        return
    
    # åˆ›å»ºLoRAè®­ç»ƒå™¨
    print("\n3. Creating LoRA trainer...")
    # å¯é€‰ï¼šå¯ç”¨ bfloat16 ä»¥è·å¾—æ›´å¥½çš„æ•ˆç‡ï¼ˆéœ€è¦ Ampere+ GPUï¼‰
    use_bfloat16 = False  # è®¾ä¸º True ä»¥å¯ç”¨ bfloat16ï¼ˆå¦‚æœGPUæ”¯æŒï¼‰
    trainer = Florence2LoRAModelTrainer(base_model_path=model_path, use_bfloat16=use_bfloat16)
    
    # å¼€å§‹LoRAè®­ç»ƒ
    print("\n4. Starting LoRA training...")
    try:
        trainer.train_lora_model(
            florence_data=florence_data,
            epochs=20,                 # è‡ªåŠ¨æ—©åœ, å¯è®¾å¤§ç‚¹
            batch_size=16,              # batch_size æ ¹æ®å†…å­˜å¤§å°è°ƒæ•´
            lr=5e-5,                   # LoRA å¯ä»¥ä½¿ç”¨ç¨é«˜çš„å­¦ä¹ ç‡
            warmup_ratio=0.1,          # å­¦ä¹ ç‡é¢„çƒ­
            # LoRA é…ç½®å‚æ•°
            lora_r=16,                 # æé«˜ç§©å‚æ•°è·å¾—æ›´å¼ºè¡¨è¾¾èƒ½åŠ›
            lora_alpha=32,             # é€šå¸¸æ˜¯ r çš„ 2 å€
            lora_dropout=0.1           # é˜²æ­¢è¿‡æ‹Ÿåˆ
        )

        print("\nâœ“ LoRA training completed successfully!")

    except Exception as e:
        print(f"\nâœ— LoRA training failed: {e}")
        import traceback
        traceback.print_exc()

def merge_existing_lora(lora_path: str, base_model_path: str, output_path: str):
    """
    åˆå¹¶ç°æœ‰çš„LoRAé€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹
    Args:
        lora_path: LoRAé€‚é…å™¨è·¯å¾„
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„  
        output_path: è¾“å‡ºè·¯å¾„
    æ³¨æ„ï¼šåªä¿å­˜æ¨¡å‹æƒé‡ï¼Œä¸ä¿å­˜processorï¼ˆprocessorä¸è®­ç»ƒæ— å…³ï¼‰
    """
    try:
        print(f"ğŸ”„ Loading LoRA adapter from {lora_path}...")
        
        # åŠ è½½åŸºç¡€æ¨¡å‹å’Œprocessorï¼ˆä»…ç”¨äºéªŒè¯ï¼‰
        from peft import PeftModel
        
        print(f"ğŸ“¥ Loading base model from {base_model_path}...")
        print(f"ğŸ” Verifying base model path exists: {os.path.exists(base_model_path)}")
        
        # å¼ºåˆ¶éªŒè¯æœ¬åœ°æ¨¡å‹æ–‡ä»¶
        if not os.path.exists(base_model_path):
            raise FileNotFoundError(f"Base model path does not exist: {base_model_path}")
        
        model_file = os.path.join(base_model_path, 'model.safetensors')
        if not os.path.exists(model_file):
            raise FileNotFoundError(f"Model file does not exist: {model_file}")
            
        # æ£€æŸ¥æ¨¡å‹å¤§å°
        expected_size = 1083916964  # æœ¬åœ°æ¨¡å‹åº”è¯¥æ˜¯è¿™ä¸ªå¤§å°
        actual_size = os.path.getsize(model_file)
        print(f"ğŸ” Base model size: {actual_size:,} bytes ({actual_size/(1024*1024):.1f}MB)")
        
        if actual_size != expected_size:
            print(f"âš ï¸  Warning: Base model size unexpected!")
            print(f"   Expected: {expected_size:,} bytes")
            print(f"   Actual: {actual_size:,} bytes")
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            trust_remote_code=True,
            torch_dtype=torch.float32,  # ä¿®å¤ï¼šå¼ºåˆ¶ä½¿ç”¨float32ä¸æœ¬åœ°æ¨¡å‹ä¿æŒä¸€è‡´
            local_files_only=True  # ç¡®ä¿ä»æœ¬åœ°åŠ è½½
        )
        
        # éªŒè¯åŠ è½½çš„æ¨¡å‹è·¯å¾„
        loaded_path = getattr(base_model.config, '_name_or_path', 'Unknown')
        print(f"ğŸ” Loaded model config path: {loaded_path}")
        if loaded_path != base_model_path:
            raise ValueError(f"Model loaded from wrong path! Expected: {base_model_path}, Got: {loaded_path}")
        
        # éªŒè¯æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in base_model.parameters())
        print(f"ğŸ” Base model parameters: {total_params:,}")
        expected_params = 270803968  # æœ¬åœ°æ¨¡å‹çš„å‚æ•°æ•°é‡
        if total_params != expected_params:
            print(f"âš ï¸  Warning: Parameter count unexpected!")
            print(f"   Expected: {expected_params:,}")
            print(f"   Actual: {total_params:,}")
        
        print("âœ“ Base model verification passed")
        
        print(f"ğŸ”— Loading and merging LoRA adapter...")
        # åŠ è½½LoRAé€‚é…å™¨
        model_with_lora = PeftModel.from_pretrained(base_model, lora_path)
        
        # åˆå¹¶æƒé‡
        merged_model = model_with_lora.merge_and_unload()
        
        # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹æƒé‡
        print(f"ğŸ’¾ Saving merged model weights to {output_path}...")
        os.makedirs(output_path, exist_ok=True)
        merged_model.save_pretrained(output_path)
        
        # å¤åˆ¶åŸºç¡€æ¨¡å‹çš„é…ç½®æ–‡ä»¶ï¼ˆç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„config.jsonï¼‰
        print(f"ğŸ“‹ Copying base model config files...")
        base_config_files = ['config.json', 'generation_config.json']
        for config_file in base_config_files:
            src_path = os.path.join(base_model_path, config_file)
            dst_path = os.path.join(output_path, config_file)
            if os.path.exists(src_path):
                shutil.copy2(src_path, dst_path)
                print(f"âœ“ Copied {config_file} from base model")
            else:
                print(f"âš ï¸  {config_file} not found in base model path")
        
        # ä¿å­˜åˆå¹¶ä¿¡æ¯
        merge_info = {
            "model_type": "florence2_merged",
            "base_model_source": base_model_path,
            "lora_adapter_source": lora_path,
            "merge_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "note": "Merged model with base model configs - ready for direct use"
        }
        
        with open(os.path.join(output_path, "merge_info.json"), "w") as f:
            json.dump(merge_info, f, indent=2)
        
        print(f"âœ“ Successfully merged LoRA adapter into complete model")
        print(f"âœ“ Merged model saved to: {output_path}")
        print(f"â„¹ï¸  Usage: AutoModelForCausalLM.from_pretrained('{output_path}')")
        
        return True
        
    except Exception as e:
        print(f"âœ— Error merging LoRA adapter: {e}")
        import traceback
        traceback.print_exc()
        return False

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="Florence2 LoRA Fine-tuning with optional model merging",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic LoRA training
  python finetune_omniparser_lora.py
  
  # LoRA training with model merging
  python finetune_omniparser_lora.py --merge
  
  # Custom merge path
  python finetune_omniparser_lora.py --merge --merge_path weights/my_merged_model
  
  # Only merge existing LoRA adapter (no training)
  python finetune_omniparser_lora.py --merge_only --lora_path weights/icon_caption_florence_lora_finetuned
        """
    )
    
    parser.add_argument(
        "--data",
        type=str,
        default="training_data/florence_format/florence_data.json",
        help="Path to training data JSON file"
    )
    
    parser.add_argument(
        "--model_path",
        type=str,
        default="weights/icon_caption_florence",
        help="Path to base Florence2 model"
    )
    
    parser.add_argument(
        "--merge",
        action="store_true",
        help="Merge LoRA adapter with base model after training"
    )
    
    parser.add_argument(
        "--merge_path",
        type=str,
        default="weights/icon_caption_florence_merged",
        help="Output path for merged model"
    )
    
    parser.add_argument(
        "--merge_only",
        action="store_true",
        help="Only merge existing LoRA adapter (skip training)"
    )
    
    parser.add_argument(
        "--lora_path",
        type=str,
        default="weights/icon_caption_florence_lora_finetuned",
        help="Path to existing LoRA adapter for merge-only mode"
    )
    
    parser.add_argument(
        "--epochs",
        type=int,
        default=20,
        help="Number of training epochs"
    )
    
    parser.add_argument(
        "--batch_size",
        type=int,
        default=16,
        help="Training batch size"
    )
    
    parser.add_argument(
        "--lr",
        type=float,
        default=5e-5,
        help="Learning rate"
    )
    
    parser.add_argument(
        "--lora_r",
        type=int,
        default=16,
        help="LoRA rank parameter"
    )
    
    parser.add_argument(
        "--lora_alpha",
        type=int,
        default=32,
        help="LoRA alpha parameter"
    )
    
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_arguments()
    
    # å¦‚æœåªæ˜¯åˆå¹¶ç°æœ‰LoRAé€‚é…å™¨
    if args.merge_only:
        print("=== LoRA Model Merging ===")
        print(f"Base model: {args.model_path}")
        print(f"LoRA adapter: {args.lora_path}")
        print(f"Output path: {args.merge_path}")
        
        if not os.path.exists(args.model_path):
            print(f"âœ— Base model path does not exist: {args.model_path}")
            exit(1)
        
        if not os.path.exists(args.lora_path):
            print(f"âœ— LoRA adapter path does not exist: {args.lora_path}")
            exit(1)
        
        success = merge_existing_lora(args.lora_path, args.model_path, args.merge_path)
        exit(0 if success else 1)
    
    # æ­£å¸¸çš„è®­ç»ƒæµç¨‹
    print("=== Florence2 LoRA Model Fine-tuning ===")
    print(f"Training data: {args.data}")
    print(f"Base model: {args.model_path}")
    print(f"Merge after training: {args.merge}")
    if args.merge:
        print(f"Merge output path: {args.merge_path}")
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import peft
        print(f"âœ“ PEFT library version: {peft.__version__}")
    except ImportError:
        print("âœ— PEFT library not found. Please install with: pip install peft")
        exit(1)
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(args.model_path):
        print(f"Error: Model path {args.model_path} does not exist!")
        print("Please ensure you have downloaded the Florence2 model weights.")
        exit(1)
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    print("\n1. Testing LoRA model loading...")
    if not test_lora_model_loading(args.model_path):
        print("LoRA model loading test failed. Please check your model weights.")
        exit(1)
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    print("\n2. Preparing training data...")
    florence_data = prepare_training_data(args.data)
    
    if not florence_data:
        print("No training data available. Please prepare your training data first.")
        print("Expected format: [{\"image_path\": \"path/to/image\", \"content\": \"description\", \"bbox\": [x1,y1,x2,y2]}, ...]")
        exit(1)
    
    # åˆ›å»ºLoRAè®­ç»ƒå™¨
    print("\n3. Creating LoRA trainer...")
    use_bfloat16 = False  # è®¾ä¸º True ä»¥å¯ç”¨ bfloat16ï¼ˆå¦‚æœGPUæ”¯æŒï¼‰
    trainer = Florence2LoRAModelTrainer(base_model_path=args.model_path, use_bfloat16=use_bfloat16)
    
    # å¼€å§‹LoRAè®­ç»ƒ
    print("\n4. Starting LoRA training...")
    try:
        trainer.train_lora_model(
            florence_data=florence_data,
            epochs=30,                 # è‡ªåŠ¨æ—©åœ, å¯è®¾å¤§ç‚¹
            batch_size=16,              # batch_size æ ¹æ®å†…å­˜å¤§å°è°ƒæ•´
            lr=5e-5,                   # LoRA å¯ä»¥ä½¿ç”¨ç¨é«˜çš„å­¦ä¹ ç‡
            warmup_ratio=0.1,          # å­¦ä¹ ç‡é¢„çƒ­
            # LoRA é…ç½®å‚æ•°
            lora_r=16,                 # æé«˜ç§©å‚æ•°è·å¾—æ›´å¼ºè¡¨è¾¾èƒ½åŠ›
            lora_alpha=32,             # é€šå¸¸æ˜¯ r çš„ 2 å€
            lora_dropout=0.05          # é˜²æ­¢è¿‡æ‹Ÿåˆ
        )

        print("\nâœ“ LoRA training completed successfully!")
        
        # å¦‚æœå¼€å¯mergeé€‰é¡¹ï¼Œåˆå¹¶å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹
        if args.merge:
            print("\n5. Merging LoRA with base model...")
            if trainer.merge_and_save_model(args.merge_path, save_quantized=True):
                print(f"âœ“ Merged model weights saved to {args.merge_path}")
                print(f"â„¹ï¸  Use with processor from weights/icon_caption_florence")
            else:
                print("âœ— Model merge failed")
                exit(1)

    except Exception as e:
        print(f"\nâœ— LoRA training failed: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
