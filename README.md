# OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent

<p align="center">
  <img src="imgs/logo.png" alt="Logo">
</p>
<!-- <a href="https://trendshift.io/repositories/12975" target="_blank"><img src="https://trendshift.io/api/badge/repositories/12975" alt="microsoft%2FOmniParser | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a> -->

[![arXiv](https://img.shields.io/badge/Paper-green)](https://arxiv.org/abs/2408.00203)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

ğŸ“¢ [[Project Page](https://microsoft.github.io/OmniParser/)] [[V2 Blog Post](https://www.microsoft.com/en-us/research/articles/omniparser-v2-turning-any-llm-into-a-computer-use-agent/)] [[Models V2](https://huggingface.co/microsoft/OmniParser-v2.0)] [[Models V1.5](https://huggingface.co/microsoft/OmniParser)] [[HuggingFace Space Demo](https://huggingface.co/spaces/microsoft/OmniParser-v2)]

**OmniParser** is a comprehensive method for parsing user interface screenshots into structured and easy-to-understand elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface.

## News

- [2025/3] We support local logging of trajecotry so that you can use OmniParser+OmniTool to build training data pipeline for your favorate agent in your domain. [Documentation WIP]
- [2025/3] We are gradually adding multi agents orchstration and improving user interface in OmniTool for better experience.
- [2025/2] We release OmniParser V2 [checkpoints](https://huggingface.co/microsoft/OmniParser-v2.0). [Watch Video](https://1drv.ms/v/c/650b027c18d5a573/EWXbVESKWo9Buu6OYCwg06wBeoM97C6EOTG6RjvWLEN1Qg?e=alnHGC)
- [2025/2] We introduce OmniTool: Control a Windows 11 VM with OmniParser + your vision model of choice. OmniTool supports out of the box the following large language models - OpenAI (4o/o1/o3-mini), DeepSeek (R1), Qwen (2.5VL) or Anthropic Computer Use. [Watch Video](https://1drv.ms/v/c/650b027c18d5a573/EehZ7RzY69ZHn-MeQHrnnR4BCj3by-cLLpUVlxMjF4O65Q?e=8LxMgX)
- [2025/1] V2 is coming. We achieve new state of the art results 39.5% on the new grounding benchmark [Screen Spot Pro](https://github.com/likaixin2000/ScreenSpot-Pro-GUI-Grounding/tree/main) with OmniParser v2 (will be released soon)! Read more details [here](https://github.com/microsoft/OmniParser/tree/master/docs/Evaluation.md).
- [2024/11] We release an updated version, OmniParser V1.5 which features 1) more fine grained/small icon detection, 2) prediction of whether each screen element is interactable or not. Examples in the demo.ipynb.
- [2024/10] OmniParser was the #1 trending model on huggingface model hub (starting 10/29/2024).
- [2024/10] Feel free to checkout our demo on [huggingface space](https://huggingface.co/spaces/microsoft/OmniParser)! (stay tuned for OmniParser + Claude Computer Use)
- [2024/10] Both Interactive Region Detection Model and Icon functional description model are released! [Hugginface models](https://huggingface.co/microsoft/OmniParser)
- [2024/09] OmniParser achieves the best performance on [Windows Agent Arena](https://microsoft.github.io/WindowsAgentArena/)!

## Install

First clone the repo, and then install environment:

```python
cd OmniParser
conda create -n "omni" python==3.12
conda activate omni
pip install -r requirements.txt
```

Ensure you have the V2 weights downloaded in weights folder (ensure caption weights folder is called icon_caption_florence). If not download them with:

```
   # download the model checkpoints to local directory OmniParser/weights/
   for f in icon_detect/{train_args.yaml,model.pt,model.yaml} icon_caption/{config.json,generation_config.json,model.safetensors}; do huggingface-cli download microsoft/OmniParser-v2.0 "$f" --local-dir weights; done
   mv weights/icon_caption weights/icon_caption_florence
```

<!-- ## [deprecated]
Then download the model ckpts files in: https://huggingface.co/microsoft/OmniParser, and put them under weights/, default folder structure is: weights/icon_detect, weights/icon_caption_florence, weights/icon_caption_blip2. 

For v1: 
convert the safetensor to .pt file. 
```python
python weights/convert_safetensor_to_pt.py

For v1.5: 
download 'model_v1_5.pt' from https://huggingface.co/microsoft/OmniParser/tree/main/icon_detect_v1_5, make a new dir: weights/icon_detect_v1_5, and put it inside the folder. No weight conversion is needed. 
``` -->

## Examples:

We put together a few simple examples in the demo.ipynb.

## Gradio Demo

To run gradio demo, simply run:

```python
python gradio_demo.py
```

## Model Weights License

For the model checkpoints on huggingface model hub, please note that icon_detect model is under AGPL license since it is a license inherited from the original yolo model. And icon_caption_blip2 & icon_caption_florence is under MIT license. Please refer to the LICENSE file in the folder of each model: https://huggingface.co/microsoft/OmniParser.

## ğŸ“š Citation

Our technical report can be found [here](https://arxiv.org/abs/2408.00203).
If you find our work useful, please consider citing our work:

```
@misc{lu2024omniparserpurevisionbased,
      title={OmniParser for Pure Vision Based GUI Agent}, 
      author={Yadong Lu and Jianwei Yang and Yelong Shen and Ahmed Awadallah},
      year={2024},
      eprint={2408.00203},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.00203}, 
}
```

## æŠ€æœ¯æ¶æ„è¯¦è§£ - ä¸‰å±‚æå–æœºåˆ¶

OmniParseré‡‡ç”¨äº†ä¸€ä¸ªä¸‰å±‚çº§è”çš„æ£€æµ‹å’Œè¯†åˆ«æ¶æ„ï¼Œåˆ†åˆ«å¤„ç†ä¸åŒç±»å‹çš„ç•Œé¢å…ƒç´ ï¼š

### 1. å®Œæ•´çš„å¤„ç†æµç¨‹

```
è¾“å…¥å›¾ç‰‡ â†’ PaddleOCRæ–‡æœ¬æ£€æµ‹ â†’ YOLOå›¾æ ‡æ£€æµ‹ â†’ é‡å å¤„ç†ä¸èåˆ â†’ Florence2è¯­ä¹‰ç†è§£ â†’ è¾“å‡ºç»“æ„åŒ–ç»“æœ
```

å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š

1. **ç¬¬ä¸€å±‚ï¼šOCRæ–‡æœ¬æ£€æµ‹ (PaddleOCR)**

   - ä½¿ç”¨PaddleOCRè¯†åˆ«å›¾åƒä¸­çš„æ‰€æœ‰æ–‡æœ¬åŒºåŸŸ
   - æå–æ–‡æœ¬å†…å®¹å’Œè¾¹ç•Œæ¡†åæ ‡
   - ç”Ÿæˆ `box_ocr_content_ocr` ç±»å‹çš„å…ƒç´ 
2. **ç¬¬äºŒå±‚ï¼šå›¾æ ‡åŒºåŸŸæ£€æµ‹ (YOLO)**

   - ä½¿ç”¨è®­ç»ƒå¥½çš„YOLOæ¨¡å‹æ£€æµ‹å¯äº¤äº’çš„å›¾æ ‡åŒºåŸŸ
   - è¾“å‡ºè¾¹ç•Œæ¡†åæ ‡ï¼Œä½†ä¸åŒ…å«è¯­ä¹‰å†…å®¹
   - æ­¤æ—¶å›¾æ ‡å…ƒç´ çš„contentä¸ºNone
3. **ç¬¬ä¸‰å±‚ï¼šé‡å å¤„ç†ä¸è¯­ä¹‰èåˆ**

   - ä½¿ç”¨`remove_overlap_new()`å‡½æ•°å¤„ç†OCRå’ŒYOLOæ£€æµ‹ç»“æœçš„é‡å 
   - å¦‚æœOCRæ–‡æœ¬åœ¨YOLOæ£€æµ‹çš„å›¾æ ‡å†…éƒ¨ï¼Œåˆ™åˆå¹¶ä¸º `box_yolo_content_ocr`
   - å¦‚æœYOLOæ£€æµ‹åˆ°çš„å›¾æ ‡æ²¡æœ‰OCRæ–‡æœ¬ï¼Œä¿ç•™ä¸º `box_yolo_content_yolo`
4. **ç¬¬å››å±‚ï¼šè¯­ä¹‰å†…å®¹ç”Ÿæˆ (Florence2)**

   - å¯¹äºcontentä¸ºNoneçš„å›¾æ ‡åŒºåŸŸï¼Œè£å‰ªå‡º64x64çš„å›¾åƒç‰‡æ®µ
   - ä½¿ç”¨Florence2æ¨¡å‹ç”Ÿæˆè¯­ä¹‰æè¿°
   - æ›´æ–°å›¾æ ‡çš„contentå­—æ®µ

### 2. ä¸‰ç§Sourceç±»å‹çš„å«ä¹‰

- **`box_ocr_content_ocr`**: çº¯OCRè¯†åˆ«çš„æ–‡æœ¬åŒºåŸŸï¼Œinteractivity=False
- **`box_yolo_content_ocr`**: YOLOæ£€æµ‹åˆ°çš„å›¾æ ‡åŒºåŸŸå†…åŒ…å«OCRæ–‡æœ¬ï¼Œåˆå¹¶äº†ä¸¤è€…ä¿¡æ¯ï¼Œinteractivity=True
- **`box_yolo_content_yolo`**: YOLOæ£€æµ‹åˆ°çš„å›¾æ ‡åŒºåŸŸï¼Œç”±Florence2ç”Ÿæˆè¯­ä¹‰æè¿°ï¼Œinteractivity=True

### 3. YOLOå¦‚ä½•è¾“å‡ºContent

YOLOæœ¬èº«åªæ£€æµ‹è¾¹ç•Œæ¡†ï¼Œä¸ç›´æ¥è¾“å‡ºcontentã€‚Contentçš„ç”Ÿæˆè¿‡ç¨‹æ˜¯ï¼š

1. **é‡å æ£€æµ‹é˜¶æ®µ**ï¼šå¦‚æœYOLOæ¡†å†…æœ‰OCRæ–‡æœ¬ï¼Œç›´æ¥ä½¿ç”¨OCRçš„æ–‡æœ¬ä½œä¸ºcontent
2. **è¯­ä¹‰ç”Ÿæˆé˜¶æ®µ**ï¼šå¯¹äºæ²¡æœ‰OCRæ–‡æœ¬çš„YOLOæ¡†ï¼Œé€šè¿‡ä»¥ä¸‹æ­¥éª¤ç”Ÿæˆcontentï¼š
   ```python
   # è£å‰ªå›¾æ ‡åŒºåŸŸ
   cropped_image = image_source[ymin:ymax, xmin:xmax, :]
   cropped_image = cv2.resize(cropped_image, (64, 64))

   # ä½¿ç”¨Florence2ç”Ÿæˆæè¿°
   inputs = processor(images=batch, text=["<CAPTION>"], return_tensors="pt")
   generated_ids = model.generate(input_ids=inputs["input_ids"], 
                                 pixel_values=inputs["pixel_values"], 
                                 max_new_tokens=20, num_beams=1)
   ```

### 4. æé«˜APKæ¢æµ‹å‡†ç¡®åº¦çš„å»ºè®®

**é—®é¢˜åˆ†æ**ï¼šå•çº¯è®­ç»ƒYOLOæ¨¡å‹æœ‰ä¸€å®šå±€é™æ€§

**æ¨èæ–¹æ¡ˆ**ï¼š

1. **æ•°æ®å¢å¼º**ï¼š

   - ä½¿ç”¨é¡¹ç›®ç°æœ‰è¾“å‡ºä½œä¸ºåŸºç¡€æ•°æ®
   - äººå·¥ä¿®æ­£è¾¹ç•Œæ¡†å’Œæ ‡ç­¾
   - å¢åŠ ç§»åŠ¨ç«¯ç•Œé¢çš„è®­ç»ƒæ ·æœ¬
2. **æ¨¡å‹ç»„åˆä¼˜åŒ–**ï¼š

   - ä¿æŒä¸‰å±‚æ¶æ„ï¼Œé‡ç‚¹ä¼˜åŒ–YOLOæ£€æµ‹å‡†ç¡®ç‡
   - é’ˆå¯¹APKç•Œé¢ç‰¹ç‚¹ï¼Œè°ƒæ•´æ£€æµ‹é˜ˆå€¼ (BOX_THRESHOLD, IoUé˜ˆå€¼)
   - è€ƒè™‘ä½¿ç”¨æ›´å¤§çš„å›¾åƒè¾“å…¥å°ºå¯¸ (imgszå‚æ•°)
3. **è®­ç»ƒç­–ç•¥**ï¼š

   ```python
   # æ•°æ®å‡†å¤‡æµç¨‹
   åŸå§‹APKæˆªå›¾ â†’ OmniParserå¤„ç† â†’ äººå·¥æ ¡æ­£ â†’ è½¬æ¢ä¸ºYOLOè®­ç»ƒæ ¼å¼ â†’ å¢é‡è®­ç»ƒ
   ```
4. **ç‰¹æ®Šä¼˜åŒ–**ï¼š

   - å¢å¼ºFlorence2æ¨¡å‹å¯¹ç§»åŠ¨ç•Œé¢å…ƒç´ çš„æè¿°èƒ½åŠ›
   - ä¼˜åŒ–OCRå‚æ•°ä»¥æ›´å¥½è¯†åˆ«ç§»åŠ¨ç«¯æ–‡æœ¬
   - è°ƒæ•´é‡å å¤„ç†çš„IoUé˜ˆå€¼ï¼Œé€‚åº”ç§»åŠ¨ç«¯ç•Œé¢ç‰¹ç‚¹

**å…³é”®ä»£ç ä½ç½®**ï¼š

- YOLOè®­ç»ƒï¼š`weights/icon_detect/`
- é‡å å¤„ç†ï¼š`util/utils.py:remove_overlap_new()`
- è¯­ä¹‰ç”Ÿæˆï¼š`util/utils.py:get_parsed_content_icon()`

### 5. æ ¸å¿ƒä»£ç ç¤ºä¾‹

**é‡å å¤„ç†é€»è¾‘**ï¼š

```python
def remove_overlap_new(boxes, iou_threshold, ocr_bbox=None):
    # OCRæ–‡æœ¬åœ¨YOLOå›¾æ ‡å†…éƒ¨çš„åˆ¤æ–­
    if is_inside(box3, box1): # ocr inside icon
        ocr_labels += box3_elem['content'] + ' '
        filtered_boxes.remove(box3_elem)
    elif is_inside(box1, box3): # icon inside ocr
        box_added = True  # ä¸æ·»åŠ æ­¤å›¾æ ‡æ¡†
        break
  
    # æ ¹æ®æ˜¯å¦æœ‰OCRæ–‡æœ¬è®¾ç½®ä¸åŒçš„source
    if ocr_labels:
        filtered_boxes.append({
            'type': 'icon', 
            'bbox': box1_elem['bbox'], 
            'interactivity': True, 
            'content': ocr_labels, 
            'source': 'box_yolo_content_ocr'
        })
    else:
        filtered_boxes.append({
            'type': 'icon', 
            'bbox': box1_elem['bbox'], 
            'interactivity': True, 
            'content': None, 
            'source': 'box_yolo_content_yolo'
        })
```

**å®Œæ•´å¤„ç†æµç¨‹**ï¼š

```python
# ä¸»å¤„ç†å‡½æ•° get_som_labeled_img() çš„æ ¸å¿ƒæ­¥éª¤
def get_som_labeled_img(image_source, model, ...):
    # 1. YOLOæ£€æµ‹å›¾æ ‡
    xyxy, logits, phrases = predict_yolo(model, image_source, ...)
  
    # 2. åˆ›å»ºOCRå’ŒYOLOå…ƒç´ 
    ocr_bbox_elem = [{'type': 'text', 'bbox': box, 'content': txt, 'source': 'box_ocr_content_ocr'} 
                     for box, txt in zip(ocr_bbox, ocr_text)]
    xyxy_elem = [{'type': 'icon', 'bbox': box, 'content': None} 
                 for box in xyxy.tolist()]
  
    # 3. å¤„ç†é‡å å¹¶èåˆ
    filtered_boxes = remove_overlap_new(xyxy_elem, iou_threshold, ocr_bbox_elem)
  
    # 4. å¯¹content=Noneçš„å›¾æ ‡ç”Ÿæˆè¯­ä¹‰æè¿°
    if use_local_semantics:
        parsed_content_icon = get_parsed_content_icon(filtered_boxes, ...)
        # å¡«å……ç©ºcontent
        for box in filtered_boxes_elem:
            if box['content'] is None:
                box['content'] = parsed_content_icon.pop(0)
```

### 6. æµç¨‹å¯è§†åŒ–

ä¸Šè¿°æµç¨‹å¯ä»¥ç”¨ä»¥ä¸‹å›¾è¡¨è¡¨ç¤ºï¼š

```
è¾“å…¥å›¾ç‰‡
â”œâ”€â”€ PaddleOCRæ–‡æœ¬æ£€æµ‹ â†’ OCRè¾¹ç•Œæ¡† + æ–‡æœ¬å†…å®¹
â””â”€â”€ YOLOå›¾æ ‡æ£€æµ‹ â†’ YOLOè¾¹ç•Œæ¡† (content=None)
                    â†“
              remove_overlap_new() é‡å å¤„ç†
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”œâ”€â”€ OCRæ–‡æœ¬åœ¨YOLOæ¡†å†…? â”€â”€â”¤
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“æ˜¯                    â†“å¦
box_yolo_content_ocr     box_yolo_content_yolo
(åˆå¹¶OCRæ–‡æœ¬)              (content=None)
                              â†“
                      Florence2è¯­ä¹‰ç”Ÿæˆ
                              â†“
                       æ›´æ–°contentå­—æ®µ
                              â†“
                      æœ€ç»ˆç»“æ„åŒ–è¾“å‡º
```

### 7. Florence2æ¨¡å‹çš„è¯¦ç»†å·¥ä½œåŸç†

Florence2æ¨¡å‹åœ¨OmniParserä¸­æ‰®æ¼”ç€å…³é”®çš„è¯­ä¹‰ç†è§£è§’è‰²ï¼Œä¸“é—¨è´Ÿè´£ä¸ºæ²¡æœ‰OCRæ–‡æœ¬çš„å›¾æ ‡åŒºåŸŸç”Ÿæˆè¯­ä¹‰æè¿°ã€‚

#### 7.1 Florence2çš„è¾“å…¥å¤„ç†

```python
def get_parsed_content_icon(filtered_boxes, starting_idx, image_source, caption_model_processor, ...):
    # 1. æå–éœ€è¦å¤„ç†çš„å›¾æ ‡åŒºåŸŸï¼ˆcontent=Noneçš„YOLOæ£€æµ‹æ¡†ï¼‰
    non_ocr_boxes = filtered_boxes[starting_idx:]  # è·³è¿‡å·²æœ‰contentçš„OCRåŒºåŸŸ
  
    # 2. è£å‰ªå¹¶é¢„å¤„ç†å›¾åƒ
    croped_pil_image = []
    for coord in non_ocr_boxes:
        # å°†ç›¸å¯¹åæ ‡è½¬æ¢ä¸ºåƒç´ åæ ‡
        xmin, xmax = int(coord[0]*width), int(coord[2]*width)
        ymin, ymax = int(coord[1]*height), int(coord[3]*height)
      
        # è£å‰ªå‡ºå›¾æ ‡åŒºåŸŸå¹¶è°ƒæ•´ä¸º64x64æ ‡å‡†å°ºå¯¸
        cropped_image = image_source[ymin:ymax, xmin:xmax, :]
        cropped_image = cv2.resize(cropped_image, (64, 64))
        croped_pil_image.append(to_pil(cropped_image))
```

#### 7.2 Florence2çš„æ¨ç†è¿‡ç¨‹

```python
    # 3. æ‰¹é‡å¤„ç†å›¾åƒï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
    model, processor = caption_model_processor['model'], caption_model_processor['processor']
  
    for i in range(0, len(croped_pil_image), batch_size):
        batch = croped_pil_image[i:i+batch_size]
      
        # 4. å‡†å¤‡è¾“å…¥ï¼ˆå›¾åƒ+æç¤ºè¯ï¼‰
        inputs = processor(
            images=batch, 
            text=["<CAPTION>"] * len(batch),  # Florence2ç‰¹æœ‰çš„æç¤ºæ ¼å¼
            return_tensors="pt", 
            do_resize=False
        ).to(device=device, dtype=torch.float16)
      
        # 5. ç”Ÿæˆè¯­ä¹‰æè¿°
        generated_ids = model.generate(
            input_ids=inputs["input_ids"],
            pixel_values=inputs["pixel_values"],
            max_new_tokens=20,  # é™åˆ¶è¾“å‡ºé•¿åº¦
            num_beams=1,        # è´ªå©ªæœç´¢
            do_sample=False     # ç¡®å®šæ€§è¾“å‡º
        )
      
        # 6. è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
        generated_texts.extend([gen.strip() for gen in generated_text])
```

#### 7.3 Florence2åœ¨ä¸åŒåœºæ™¯ä¸­çš„åº”ç”¨

**è¾“å…¥ç¤ºä¾‹**ï¼š64x64çš„å›¾æ ‡è£å‰ªå›¾åƒ

- ğŸ“ æ–‡ä»¶å¤¹å›¾æ ‡ â†’ "folder"
- âš™ï¸ è®¾ç½®é½¿è½® â†’ "settings gear icon"
- ğŸ” æœç´¢æ”¾å¤§é•œ â†’ "magnifying glass search"
- â• æ·»åŠ æŒ‰é’® â†’ "plus add button"

**å®é™…è¾“å‡ºç¤ºä¾‹**ï¼ˆæ¥è‡ªé¡¹ç›®æ•°æ®ï¼‰ï¼š

```json
{
  "type": "icon",
  "bbox": [0.8391380906105042, 0.16333389282226562, 0.9413251876831055, 0.19683626294136047],
  "interactivity": true,
  "content": "a loading or buffering indicator.",
  "source": "box_yolo_content_yolo"
}
```

#### 7.4 Florence2æ¨¡å‹çš„ä¼˜åŠ¿

1. **ä¸“é—¨ä¼˜åŒ–**ï¼šFlorence2æ˜¯å¾®è½¯ä¸“é—¨ä¸ºè§†è§‰ç†è§£ä»»åŠ¡ä¼˜åŒ–çš„å¤šæ¨¡æ€æ¨¡å‹
2. **é«˜æ•ˆæ¨ç†**ï¼šæ”¯æŒæ‰¹é‡å¤„ç†ï¼Œ64x64å°å›¾åƒå¤„ç†é€Ÿåº¦å¿«
3. **è¯­ä¹‰ä¸°å¯Œ**ï¼šèƒ½å¤Ÿè¯†åˆ«å›¾æ ‡çš„åŠŸèƒ½å«ä¹‰ï¼Œè€Œä¸ä»…ä»…æ˜¯è§†è§‰ç‰¹å¾
4. **ä¸Šä¸‹æ–‡ç†è§£**ï¼šç»“åˆUIç•Œé¢çš„ä¸Šä¸‹æ–‡ç”Ÿæˆæ›´å‡†ç¡®çš„æè¿°

#### 7.5 Florence2 vs OCRçš„åä½œå…³ç³»


| å¤„ç†å¯¹è±¡     | å¤„ç†æ–¹å¼          | è¾“å‡ºç‰¹ç‚¹              | Sourceæ ‡è®°            |
| ------------ | ----------------- | --------------------- | --------------------- |
| æ–‡æœ¬åŒºåŸŸ     | PaddleOCRç›´æ¥è¯†åˆ« | ç²¾ç¡®çš„æ–‡å­—å†…å®¹        | box_ocr_content_ocr   |
| æœ‰æ–‡å­—çš„å›¾æ ‡ | OCR+YOLOèåˆ      | "æ–‡å­—å†…å®¹ + å›¾æ ‡å±æ€§" | box_yolo_content_ocr  |
| çº¯å›¾æ ‡åŒºåŸŸ   | Florence2è¯­ä¹‰ç†è§£ | åŠŸèƒ½æ€§æè¿°            | box_yolo_content_yolo |

**å…³é”®å·®å¼‚**ï¼š

- **OCR**ï¼šè¯†åˆ«"what is written"ï¼ˆå†™äº†ä»€ä¹ˆï¼‰
- **Florence2**ï¼šç†è§£"what does it do"ï¼ˆåšä»€ä¹ˆç”¨çš„ï¼‰

----
New version

# OmniParser Chat Session Summary

## ğŸ¯ Main Question

User asked about the **three-layer extraction mechanism** in OmniParser project, specifically:

- What is the actual extraction flow?
- How does YOLO output content?
- Training strategies for better APK detection accuracy

## ğŸ” User's Initial Understanding (Corrected)

**Thought flow was:** Input image â†’ extract positions â†’ PaddleOCR â†’ YOLO â†’ final recognition â†’ concatenate results

## âœ… Technical Analysis & Corrections

### OmniParser's Actual Architecture

**Real flow:** Input image â†’ [PaddleOCR + YOLO parallel] â†’ overlap processing/fusion â†’ Florence2 semantic understanding â†’ structured output

### Three-Layer Extraction Mechanism

1. **Layer 1:** PaddleOCR text detection (generates `box_ocr_content_ocr`)
2. **Layer 2:** YOLO icon detection (detects bounding boxes)
3. **Layer 3:** Overlap processing + Florence2 semantic understanding

### Three Source Types in Output

- `box_ocr_content_ocr`: Pure OCR text regions (interactivity=False)
- `box_yolo_content_ocr`: YOLO-detected icons containing OCR text (interactivity=True)
- `box_yolo_content_yolo`: YOLO-detected icons with Florence2-generated descriptions (interactivity=True)

### YOLO Content Generation Clarification

- **YOLO only detects bounding boxes** - doesn't directly output content
- Content comes from:
  - **Overlap detection:** Uses OCR text when YOLO box overlaps with OCR region
  - **Semantic generation:** Florence2 processes 64x64 cropped images for descriptions

## ğŸ› ï¸ Training Solution Created

### Files Created

#### 1. `finetune_omniparser_models.py` - Main Training Script

- `YOLOTrainer` class for training icon detection
- `Florence2Trainer` class for training icon captioning
- `OmniParserDatasetConverter` for data format conversion
- Support for training both models separately or together

#### 2. `collect_training_data.py` - Data Collection Script

- `TrainingDataCollector` class using existing OmniParser to process images
- Manual correction interface via CSV export
- Batch processing capabilities

#### 3. `TRAINING_GUIDE.md` - Comprehensive Guide

- Data format specifications (YOLO format for detection, JSON for Florence2)
- Step-by-step training workflow
- APK-specific optimization strategies
- Troubleshooting guide

#### 4. `example_training_workflow.py` - Demonstration Script

Complete workflow example showing how to use all components

### Training Workflow

1. **Collect raw images** â†’ process with existing OmniParser â†’ generate training data
2. **Optional manual correction** via CSV editing
3. **Train models:** YOLO for detection + Florence2 for captioning
4. **Integration:** YOLO detects â†’ Florence2 describes

## ğŸ¯ APK Optimization Recommendations

- Use project output + manual correction for training data
- Maintain three-layer architecture rather than training only YOLO
- Adjust detection thresholds and IoU parameters for mobile interfaces
- Include diverse mobile interface samples in training data

## ğŸ› Critical Issue: Florenceæ¨¡å‹è®­ç»ƒé—®é¢˜

### é—®é¢˜ç—‡çŠ¶
ç»è¿‡finetuneçš„Florenceæ¨¡å‹è¾“å‡ºæ··ä¹±ï¼š
- **Nativeè¾“å‡º**ï¼šå‡†ç¡®çš„UIæè¿°ï¼ˆ`'M0,0L9,0 4.5,5z'`, `'P: 0:1'`, `'tool for cutting'`ï¼‰
- **Finetunedè¾“å‡º**ï¼šä¸ç›¸å…³æè¿°ï¼ˆ`"person's face"`, `"camera in hand"`, `"No object detected"`ï¼‰

### æ ¹æœ¬åŸå› 
1. **ç¾éš¾æ€§é—å¿˜**ï¼šå­¦ä¹ ç‡è¿‡é«˜ã€ç¼ºä¹å±‚å†»ç»“ç­–ç•¥
2. **æ¨¡å‹ç›®æ ‡é€€åŒ–**ï¼šä»UIä¸“ä¸šç†è§£é€€åŒ–ä¸ºé€šç”¨å›¾åƒæè¿°
3. **è®­ç»ƒç­–ç•¥é—®é¢˜**ï¼šæ‰€æœ‰å±‚åŒæ—¶è®­ç»ƒï¼Œç ´åäº†é¢„è®­ç»ƒçŸ¥è¯†

### è§£å†³æ–¹æ¡ˆ
**åˆ›å»ºäº†ä¿å®ˆè®­ç»ƒç­–ç•¥** (`finetune_omniparser_models_fixed.py`)ï¼š
- **å±‚å†»ç»“**ï¼šå†»ç»“vision encoderï¼Œåªè®­ç»ƒé¡¶å±‚language model
- **æ›´å°å­¦ä¹ ç‡**ï¼š2e-6 (åŸæ¥1e-5)
- **UIä¸“ç”¨prompt**ï¼šä¿®å¤`<CAPTION>`æ ¼å¼é”™è¯¯ï¼Œæ·»åŠ UIä¸Šä¸‹æ–‡åˆ°ç­”æ¡ˆ
- **Early stopping**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
- **æ¢¯åº¦è£å‰ª**ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹
- **ğŸ”§ ä½¿ç”¨æœ¬åœ°æ¨¡å‹**ï¼šåŸºäº`weights/icon_caption_florence_finetuned`è€Œéåœ¨çº¿æ¨¡å‹

## ğŸ¯ æ•°æ®å¹³è¡¡åŠŸèƒ½ä¼˜åŒ–

### é—®é¢˜å‘ç°
ç”¨æˆ·å‘ç°è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸å¹³è¡¡é—®é¢˜ï¼šä¿®æ”¹çš„æ•°æ®å¾ˆå°‘ï¼ŒåŸå§‹æ•°æ®å¾ˆå¤šï¼Œå¯¼è‡´æ¨¡å‹è¢«å¤§é‡æ—§æ•°æ®ä¸»å¯¼ã€‚

### è§£å†³æ–¹æ¡ˆ
**æ·»åŠ äº†æ•°æ®å¹³è¡¡åŠŸèƒ½åˆ° `collect_training_data.py`**ï¼š

#### æ–°åŠŸèƒ½ç‰¹æ€§
- **è‡ªåŠ¨ç»Ÿè®¡ä¿®æ”¹æ¯”ä¾‹**ï¼šæ£€æŸ¥æœ‰å¤šå°‘æ•°æ®è¢«å®é™…ä¿®æ”¹äº†
- **æ•°æ®åˆ†å¸ƒåˆ†æ**ï¼šæ˜¾ç¤ºä¿®æ”¹vsæœªä¿®æ”¹æ•°æ®çš„è¯¦ç»†ç»Ÿè®¡
- **æ™ºèƒ½æ•°æ®å¹³è¡¡**ï¼šæ ¹æ®`old_percentage`å‚æ•°éšæœºåˆ é™¤å¤šä½™çš„æœªä¿®æ”¹æ•°æ®
- **å¯é…ç½®æ¯”ä¾‹**ï¼šç”¨æˆ·å¯ä»¥æŒ‡å®šä¿ç•™å¤šå°‘ç™¾åˆ†æ¯”çš„åŸå§‹æ•°æ®

#### ä½¿ç”¨æ–¹æ³•
```bash
# é»˜è®¤ä¿ç•™50%æœªä¿®æ”¹æ•°æ®
python collect_training_data.py --output_dir ./data --apply_corrections

# åªä¿ç•™20%æœªä¿®æ”¹æ•°æ®ï¼ˆæ¨èç”¨äºå¤§é‡åŸå§‹æ•°æ®åœºæ™¯ï¼‰
python collect_training_data.py --output_dir ./training_data --apply_corrections --old_percentage 20

# ä¿ç•™80%æœªä¿®æ”¹æ•°æ®ï¼ˆé€‚åˆä¿®æ”¹æ•°æ®å¾ˆå¤šçš„åœºæ™¯ï¼‰
python collect_training_data.py --output_dir ./data --apply_corrections --old_percentage 80
```

#### è¾“å‡ºç¤ºä¾‹
```
ğŸ“Š Data Distribution Analysis:
  Modified elements: 45 (8.2%)
  Unchanged elements: 505 (91.8%)
  Total elements: 550

ğŸ¯ Data Balancing:
  Target unchanged elements: 110 (20%)
  Randomly removing 395 unchanged elements

âœ… Final Data Distribution:
  Modified elements: 45 (29.0%)
  Unchanged elements: 110 (71.0%)
  Total elements: 155
```

## ğŸ“ Key Architecture Files Analyzed

- `util/omniparser.py` - Main OmniParser class
- `util/utils.py` - Core processing functions
- `weights/icon_detect/` - YOLO model directory
- `weights/icon_caption_florence_finetuned/` - Florence2 model directory

## ğŸ‰ Outcome

Created a complete training pipeline that:

- âœ… Correctly understands OmniParser's three-layer architecture
- âœ… Provides tools for collecting and preparing training data
- âœ… Supports training both YOLO detection and Florence2 captioning models
- âœ… Includes comprehensive documentation and examples
- âœ… Focuses on APK interface detection improvements
- âœ… **æ–°å¢ï¼šæ•°æ®å¹³è¡¡åŠŸèƒ½** - æ™ºèƒ½è°ƒæ•´è®­ç»ƒæ•°æ®åˆ†å¸ƒ

## ğŸ’¡ Key Insights

- OmniParser uses **parallel processing** (PaddleOCR + YOLO), not sequential
- **YOLO only detects**, content generation is separate (overlap + Florence2)
- Training both models together maintains the **three-layer synergy**
- **Manual correction** capability is crucial for high-quality training data
- **æ•°æ®å¹³è¡¡** å¯¹é˜²æ­¢æ¨¡å‹è¢«æ—§æ•°æ®ä¸»å¯¼è‡³å…³é‡è¦

## ğŸ”§ Florence2æ¨¡å‹æ¶æ„ä¸å¾®è°ƒè¯¦è§£

### ä»€ä¹ˆæ˜¯Processorï¼Ÿ

**Processor**æ˜¯Transformersåº“ä¸­çš„é¢„å¤„ç†ç»„ä»¶ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

1. **å›¾åƒé¢„å¤„ç†å™¨(Image Processor)**ï¼š
   - è´Ÿè´£å›¾åƒçš„æ ‡å‡†åŒ–ã€resizeã€normalizationç­‰
   - å°†PILå›¾åƒè½¬æ¢ä¸ºæ¨¡å‹å¯æ¥å—çš„tensoræ ¼å¼
   - å¤„ç†å›¾åƒçš„é€šé“é¡ºåº(RGB/BGR)å’Œæ•°å€¼èŒƒå›´

2. **æ–‡æœ¬åˆ†è¯å™¨(Tokenizer)**ï¼š
   - å°†æ–‡æœ¬è½¬æ¢ä¸ºtoken IDs
   - å¤„ç†ç‰¹æ®Štokenï¼ˆå¦‚`<CAPTION>`ã€`<BOS>`ã€`<EOS>`ç­‰ï¼‰
   - ç®¡ç†è¯æ±‡è¡¨å’Œç¼–ç è§„åˆ™

3. **è¾“å…¥æ ¼å¼åŒ–å™¨**ï¼š
   - å°†å›¾åƒå’Œæ–‡æœ¬ç»„åˆæˆæ¨¡å‹è¾“å…¥æ ¼å¼
   - å¤„ç†batch paddingå’Œattention mask
   - ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®

```python
# Processorçš„å…¸å‹å·¥ä½œæµç¨‹
processor = AutoProcessor.from_pretrained("microsoft/Florence-2-base")
inputs = processor(
    images=[image],           # PILå›¾åƒåˆ—è¡¨
    text=["<CAPTION>"],       # æç¤ºè¯åˆ—è¡¨
    return_tensors="pt",      # è¿”å›PyTorch tensor
    do_resize=False          # æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œresize
)
# è¾“å‡º: {'input_ids': tensor, 'pixel_values': tensor, 'attention_mask': tensor}
```

### ä¸ºä»€ä¹ˆProcessorä»åœ¨çº¿ä¸‹è½½ï¼Ÿ

è¿™æ˜¯**æ ‡å‡†çš„AIæ¨¡å‹è®¾è®¡æ¨¡å¼**ï¼š

#### åŸå› åˆ†æ
1. **å…¼å®¹æ€§ä¿è¯**ï¼š
   - Processorå®šä¹‰äº†æ•°æ®é¢„å¤„ç†çš„æ ‡å‡†æ ¼å¼
   - å¾®è°ƒè¿‡ç¨‹ä¸­é€šå¸¸ä¸æ”¹å˜è¾“å…¥è¾“å‡ºæ ¼å¼
   - ä½¿ç”¨æ ‡å‡†processorç¡®ä¿ä¸åŸå§‹æ¨¡å‹å…¼å®¹

2. **ç¨³å®šæ€§è€ƒè™‘**ï¼š
   - Tokenizerçš„è¯æ±‡è¡¨å’Œç¼–ç è§„åˆ™ä¿æŒä¸å˜
   - é¿å…å› é¢„å¤„ç†å˜åŒ–å¯¼è‡´çš„æ¨ç†é”™è¯¯
   - ç¡®ä¿å¾®è°ƒåçš„æ¨¡å‹èƒ½æ­£ç¡®å¤„ç†è¾“å…¥

3. **æ–‡ä»¶å¤§å°ä¼˜åŒ–**ï¼š
   - é¿å…åœ¨æ¯ä¸ªå¾®è°ƒæ¨¡å‹ä¸­é‡å¤å­˜å‚¨ç›¸åŒçš„processoræ–‡ä»¶
   - å‡å°‘æ¨¡å‹åˆ†å‘çš„å­˜å‚¨å¼€é”€

#### OmniParserçš„å…·ä½“å®ç°
```python
# åŸå§‹é¡¹ç›®çš„è®¾è®¡æ¨¡å¼ (util/utils.py:78)
processor = AutoProcessor.from_pretrained("microsoft/Florence-2-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)
```

### ä¹‹å‰è®­ç»ƒé—®é¢˜çš„æ ¹æœ¬åŸå› 

#### é—®é¢˜ç—‡çŠ¶
```bash
# é”™è¯¯çš„å›é€€é€»è¾‘å¯¼è‡´ä½¿ç”¨åœ¨çº¿æ¨¡å‹
Loading Florence2 model from local path: weights/icon_caption_florence
Local processor not found, using base Florence2 processor  # âœ“ æ­£å¸¸
Error loading local model: [æŸä¸ªé”™è¯¯]                        # âœ— è§¦å‘å›é€€
Trying to load from HuggingFace hub as fallback...          # âœ— é”™è¯¯å›é€€
```

#### æ ¹æœ¬åŸå› åˆ†æ
1. **é”™è¯¯çš„å¼‚å¸¸å¤„ç†**ï¼š
   - ä»»ä½•åŠ è½½æœ¬åœ°æ¨¡å‹æ—¶çš„å°é”™è¯¯éƒ½ä¼šè§¦å‘å®Œå…¨å›é€€
   - å›é€€é€»è¾‘ç›´æ¥åŠ è½½`microsoft/Florence-2-base`è€Œéæœ¬åœ°æ¨¡å‹
   - å¯¼è‡´å®é™…è®­ç»ƒçš„æ˜¯åŸºç¡€æ¨¡å‹è€Œéæœ¬åœ°å¾®è°ƒæ¨¡å‹

2. **è®¾å¤‡å…¼å®¹æ€§é—®é¢˜**ï¼š
   - CUDAç¯å¢ƒä¸‹çš„dtypeä¸åŒ¹é…
   - `local_files_only=True`åœ¨æŸäº›æƒ…å†µä¸‹è¿‡äºä¸¥æ ¼
   - ç¼ºå°‘é€‚å½“çš„é”™è¯¯åŒºåˆ†æœºåˆ¶

#### ä¿®å¤æ–¹æ¡ˆ
```python
# ä¿®å¤åçš„æ­£ç¡®é€»è¾‘
try:
    # 1. å§‹ç»ˆä»æ ‡å‡†ä½ç½®åŠ è½½processor (æ­£ç¡®åšæ³•)
    self.processor = AutoProcessor.from_pretrained("microsoft/Florence-2-base", trust_remote_code=True)
    
    # 2. ä¸“é—¨åŠ è½½æœ¬åœ°æ¨¡å‹æƒé‡ (å…³é”®ä¿®å¤)
    self.model = AutoModelForCausalLM.from_pretrained(
        self.base_model_path,           # æœ¬åœ°è·¯å¾„
        torch_dtype=torch.float16,     # é€‚å½“çš„æ•°æ®ç±»å‹
        trust_remote_code=True,
        local_files_only=True          # å¼ºåˆ¶æœ¬åœ°åŠ è½½
    ).to(self.device)
    
except Exception as e:
    # 3. åªæœ‰åœ¨æœ¬åœ°æ–‡ä»¶çœŸæ­£ç¼ºå¤±æ—¶æ‰å›é€€
    if "does not appear to have a file named" in str(e):
        # å›é€€åˆ°åŸºç¡€æ¨¡å‹
    else:
        # å…¶ä»–é”™è¯¯åº”è¯¥æŠ›å‡ºï¼Œè€Œä¸æ˜¯é™é»˜å›é€€
        raise e
```

### å¾®è°ƒä¸»è¦å½±å“çš„æ–‡ä»¶

#### æ ¸å¿ƒæ–‡ä»¶å˜åŒ–
1. **model.safetensors** (ä¸»è¦å˜åŒ–)
   - **å¤§å°**ï¼šçº¦1GB (270Må‚æ•° Ã— 4å­—èŠ‚/å‚æ•°)
   - **å†…å®¹**ï¼šæ¨¡å‹çš„æƒé‡å‚æ•°
   - **å˜åŒ–**ï¼šå¾®è°ƒè¿‡ç¨‹ä¸­æƒé‡ä¼šæ ¹æ®è®­ç»ƒæ•°æ®è°ƒæ•´
   - **å½±å“**ï¼šç›´æ¥å†³å®šæ¨¡å‹çš„é¢„æµ‹è¡Œä¸º

2. **config.json** (åŸºæœ¬ä¸å˜)
   - **å†…å®¹**ï¼šæ¨¡å‹æ¶æ„é…ç½®
   - **åŒ…å«**ï¼šå±‚æ•°ã€éšè—å±‚å¤§å°ã€æ³¨æ„åŠ›å¤´æ•°ç­‰
   - **å˜åŒ–é¢‘ç‡**ï¼šå‡ ä¹ä¸å˜ï¼Œé™¤éæ”¹å˜æ¨¡å‹æ¶æ„
   - **ä½œç”¨**ï¼šå‘Šè¯‰æ¡†æ¶å¦‚ä½•æ„å»ºæ¨¡å‹ç»“æ„

3. **generation_config.json** (åŸºæœ¬ä¸å˜)
   - **å†…å®¹**ï¼šç”Ÿæˆå‚æ•°çš„é»˜è®¤é…ç½®
   - **åŒ…å«**ï¼šmax_lengthã€num_beamsã€temperatureç­‰
   - **å˜åŒ–é¢‘ç‡**ï¼šå¾ˆå°‘å˜åŒ–
   - **ä½œç”¨**ï¼šæ§åˆ¶æ¨ç†æ—¶çš„ç”Ÿæˆè¡Œä¸º

#### æ–‡ä»¶å˜åŒ–è¯¦ç»†åˆ†æ
```json
// config.json - æ¶æ„é…ç½® (åŸºæœ¬ä¸å˜)
{
  "model_type": "florence2",
  "vision_config": {...},      // è§†è§‰ç¼–ç å™¨é…ç½®
  "text_config": {...},        // è¯­è¨€æ¨¡å‹é…ç½®
  "projection_dim": 768,       // æŠ•å½±å±‚ç»´åº¦
  "torch_dtype": "float32"     // é»˜è®¤æ•°æ®ç±»å‹
}

// generation_config.json - ç”Ÿæˆé…ç½® (åŸºæœ¬ä¸å˜)
{
  "max_length": 20,           // æœ€å¤§ç”Ÿæˆé•¿åº¦
  "num_beams": 3,             // beam searchæ•°é‡
  "no_repeat_ngram_size": 3,  // é˜²é‡å¤n-gramå¤§å°
  "early_stopping": true      // æ—©åœç­–ç•¥
}
```

#### æƒé‡æ–‡ä»¶çš„å±‚çº§ç»“æ„
```
model.safetensors å†…éƒ¨ç»“æ„:
â”œâ”€â”€ vision_model.*              # è§†è§‰ç¼–ç å™¨ (é€šå¸¸å†»ç»“)
â”‚   â”œâ”€â”€ patch_embed.*
â”‚   â”œâ”€â”€ stages.*
â”‚   â””â”€â”€ norm.*
â”œâ”€â”€ language_model.*            # è¯­è¨€æ¨¡å‹ (ä¸»è¦å¾®è°ƒç›®æ ‡)
â”‚   â”œâ”€â”€ model.embed_tokens.*
â”‚   â”œâ”€â”€ model.layers.*
â”‚   â””â”€â”€ lm_head.*              # è¾“å‡ºå±‚ (é‡ç‚¹å¾®è°ƒ)
â””â”€â”€ projector.*                # è§†è§‰-è¯­è¨€æŠ•å½±å±‚
```

### å¾®è°ƒç­–ç•¥çš„å±‚çº§æ§åˆ¶

#### ä¿å®ˆå¾®è°ƒç­–ç•¥ (æ¨è)
```python
# å†»ç»“è§†è§‰ç¼–ç å™¨ (ä¿æŒè§†è§‰ç†è§£èƒ½åŠ›)
for name, param in model.named_parameters():
    if 'vision_model' in name:
        param.requires_grad = False

# åªå¾®è°ƒè¯­è¨€æ¨¡å‹çš„é¡¶å±‚
trainable_keywords = [
    'language_model.lm_head',      # è¾“å‡ºå±‚ (å¿…é¡»å¾®è°ƒ)
    'language_model.model.layers.5', # æœ€åä¸€å±‚transformer
    'language_model.model.layers.4', # å€’æ•°ç¬¬äºŒå±‚
    'projector'                    # æŠ•å½±å±‚
]
```

#### å¾®è°ƒæ•ˆæœéªŒè¯
```python
# å¾®è°ƒå‰ï¼šé€šç”¨å›¾åƒæè¿°
Input: 64x64 UI icon image
Output: "a picture of something"

# å¾®è°ƒåï¼šUIä¸“ç”¨æè¿°  
Input: 64x64 UI icon image
Output: "settings gear icon" / "close button" / "menu hamburger"
```

### é…ç½®æ–‡ä»¶çš„ä½œç”¨æœºåˆ¶

#### æ¨¡å‹åŠ è½½æµç¨‹
```python
# 1. è¯»å–config.jsonæ„å»ºæ¨¡å‹æ¶æ„
config = AutoConfig.from_pretrained(model_path)
model = Florence2ForConditionalGeneration(config)

# 2. åŠ è½½model.safetensorså¡«å……æƒé‡
state_dict = load_file(f"{model_path}/model.safetensors")
model.load_state_dict(state_dict)

# 3. åº”ç”¨generation_config.jsonçš„é»˜è®¤å‚æ•°
gen_config = GenerationConfig.from_pretrained(model_path)
model.generation_config = gen_config
```

#### ä¸ºä»€ä¹ˆé…ç½®æ–‡ä»¶åŸºæœ¬ä¸å˜
1. **æ¶æ„ç¨³å®šæ€§**ï¼šæ¨¡å‹çš„åŸºç¡€æ¶æ„åœ¨å¾®è°ƒä¸­ä¿æŒä¸å˜
2. **å…¼å®¹æ€§è¦æ±‚**ï¼šé…ç½®å˜åŒ–å¯èƒ½ç ´åä¸processorçš„å…¼å®¹æ€§
3. **ç”Ÿæˆè´¨é‡**ï¼šåŸå§‹çš„ç”Ÿæˆå‚æ•°é€šå¸¸å·²ç»è¿‡ä¼˜åŒ–
4. **è¿ç§»å­¦ä¹ åŸç†**ï¼šåªæ”¹å˜æƒé‡ï¼Œä¿æŒç»“æ„ä¸å˜

### æ€»ç»“è¦ç‚¹

| ç»„ä»¶ | æ¥æº | å˜åŒ–é¢‘ç‡ | ä½œç”¨ |
|------|------|----------|------|
| **Processor** | åœ¨çº¿æ ‡å‡†ç‰ˆæœ¬ | ä»ä¸ | æ•°æ®é¢„å¤„ç† |
| **model.safetensors** | æœ¬åœ°å¾®è°ƒç‰ˆæœ¬ | æ¯æ¬¡è®­ç»ƒ | æ¨¡å‹æƒé‡ |
| **config.json** | æœ¬åœ°/ç»§æ‰¿ | å‡ ä¹ä¸å˜ | æ¶æ„å®šä¹‰ |
| **generation_config.json** | æœ¬åœ°/ç»§æ‰¿ | å¾ˆå°‘å˜ | ç”Ÿæˆå‚æ•° |

è¿™ç§è®¾è®¡ç¡®ä¿äº†ï¼š
- **å…¼å®¹æ€§**ï¼šprocessoræ ‡å‡†åŒ–ä¿è¯è¾“å…¥è¾“å‡ºæ ¼å¼ä¸€è‡´
- **å¯è®­ç»ƒæ€§**ï¼šæƒé‡æ–‡ä»¶åŒ…å«æ‰€æœ‰å¯å­¦ä¹ å‚æ•°  
- **ç¨³å®šæ€§**ï¼šé…ç½®æ–‡ä»¶æä¾›ç¨³å®šçš„æ¨¡å‹è¡Œä¸º
- **æ•ˆç‡**ï¼šé¿å…é‡å¤å­˜å‚¨ç›¸åŒçš„é¢„å¤„ç†ç»„ä»¶

## ğŸ“š é«˜çº§ä¸»é¢˜

### å¾®è°ƒæ–¹æ³•ä¸æŠ€æœ¯è¯¦è§£

æœ¬é¡¹ç›®æä¾›è¯¦ç»†çš„å¾®è°ƒæŠ€æœ¯æ–‡æ¡£ï¼Œæ¶µç›–ï¼š

- **å½“å‰å¾®è°ƒæ–¹æ³•åˆ†æ**: å‚æ•°é«˜æ•ˆå¾®è°ƒ (Parameter-Efficient Fine-tuning)
- **LoRA æŠ€æœ¯è¯¦è§£**: ä½ç§©é€‚åº” (Low-Rank Adaptation) åŸç†ä¸å®ç°
- **å±‚å†»ç»“ç­–ç•¥**: å¦‚ä½•ç¡®å®šå†»ç»“å“ªäº›å±‚ä»¥åŠé€‰æ‹©ä¾æ®
- **æ€§èƒ½å¯¹æ¯”**: ä¸åŒå¾®è°ƒæ–¹æ³•çš„æ•ˆæœä¸èµ„æºæ¶ˆè€—åˆ†æ
- **å®è·µå»ºè®®**: é’ˆå¯¹ä¸åŒåœºæ™¯çš„å¾®è°ƒç­–ç•¥æ¨è

ğŸ“– **è¯¦ç»†æ–‡æ¡£**: [FINETUNING_METHODS_README.md](FINETUNING_METHODS_README.md)

### å¿«é€Ÿäº†è§£

| å¾®è°ƒæ–¹æ³• | å¯è®­ç»ƒå‚æ•° | å†…å­˜éœ€æ±‚ | é€‚ç”¨åœºæ™¯ |
|----------|------------|----------|----------|
| **å½“å‰æ–¹æ³•** (å±‚å†»ç»“) | 5-10% | ä¸­ç­‰ | èµ„æºå—é™ï¼Œå¿«é€ŸéªŒè¯ |
| **LoRA** | 0.1-1% | æä½ | å¤§è§„æ¨¡éƒ¨ç½²ï¼Œå¤šä»»åŠ¡é€‚é… |
| **å…¨å‚æ•°å¾®è°ƒ** | 100% | æé«˜ | å……è¶³èµ„æºï¼Œæœ€ä½³æ€§èƒ½ |

é€šè¿‡é˜…è¯»è¯¦ç»†æ–‡æ¡£ï¼Œæ‚¨å°†äº†è§£ï¼š
- ä¸ºä»€ä¹ˆé€‰æ‹©å†»ç»“ `language_model.lm_head`ã€`language_model.model.layers.4-5` å’Œ `projector`
- LoRA å¦‚ä½•é€šè¿‡ä½ç§©åˆ†è§£å®ç°å‚æ•°é«˜æ•ˆè®­ç»ƒ
- å¦‚ä½•æ ¹æ®æ‚¨çš„å…·ä½“éœ€æ±‚é€‰æ‹©æœ€é€‚åˆçš„å¾®è°ƒç­–ç•¥

## BOX_THRESHOLD å’Œ iou_threshold åœ¨ YOLO ä¸­çš„ä½œç”¨

BOX_THRESHOLD (ç½®ä¿¡åº¦é˜ˆå€¼)

ä½œç”¨: æ§åˆ¶ YOLO ç›®æ ‡æ£€æµ‹çš„ç½®ä¿¡åº¦è¿‡æ»¤ï¼Œå†³å®šå“ªäº›æ£€æµ‹ç»“æœè¢«ä¿ç•™

æœºåˆ¶:
- YOLO æ£€æµ‹ç½®ä¿¡åº¦ < BOX_THRESHOLD çš„ç»“æœè¢«ä¸¢å¼ƒ
- ä¼ é€’ç»™ YOLO æ¨¡å‹çš„ conf å‚æ•°

é»˜è®¤å€¼:
- å‡½æ•°é»˜è®¤: 0.01
- åº”ç”¨é»˜è®¤: 0.05 (demosã€serversã€UI)
- èŒƒå›´: 0.01-1.0

è°ƒæ•´åœºæ™¯:
- é™ä½ (0.01-0.03): æ£€æµ‹æ›´å¤š UI å…ƒç´ ï¼Œé€‚åˆå…¨é¢è§£æ
- æ ‡å‡† (0.05): å¹³è¡¡å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ï¼Œæœ€å¸¸ç”¨
- æé«˜ (0.1-0.3): åªä¿ç•™é«˜ç½®ä¿¡åº¦æ£€æµ‹ï¼Œå‡å°‘è¯¯æ£€

iou_threshold (äº¤å¹¶æ¯”é˜ˆå€¼)

ä½œç”¨: æ§åˆ¶é‡å æ£€æµ‹çš„ç§»é™¤ï¼Œç”¨äºä¸¤ä¸ªå±‚é¢ï¼š
1. YOLO å†…éƒ¨ NMS (éæå¤§å€¼æŠ‘åˆ¶)
2. OCR å’Œ YOLO æ£€æµ‹ç»“æœçš„èåˆ

é»˜è®¤å€¼:
- YOLO NMS: 0.7
- é‡å ç§»é™¤: 0.7-0.9
- Gradio ç•Œé¢: 0.1 (ç”¨æˆ·å¯è°ƒ)

èåˆé€»è¾‘:
OCR æ–‡æœ¬åœ¨ YOLO å›¾æ ‡å†… â†’ åˆå¹¶ä¸º 'box_yolo_content_ocr'
YOLO å›¾æ ‡åœ¨ OCR æ–‡æœ¬å†… â†’ è·³è¿‡è¯¥å›¾æ ‡
æ— é‡å  â†’ ä¿æŒç‹¬ç«‹å…ƒç´ 

è°ƒæ•´åœºæ™¯:
- é™ä½ (0.1-0.3): æ›´æ¿€è¿›çš„é‡å ç§»é™¤ï¼Œé€‚åˆå¯†é›† UI
- æé«˜ (0.7-0.9): ä¿ç•™æ›´å¤šæ£€æµ‹ç»“æœï¼Œé€‚åˆç¨€ç– UI

æ¨èè®¾ç½®

| åœºæ™¯     | BOX_THRESHOLD | iou_threshold |
|--------|---------------|---------------|
| å¯†é›†å¤æ‚ç•Œé¢ | 0.03-0.05     | 0.3-0.5       |
| æ ‡å‡†ç•Œé¢   | 0.05          | 0.7           |
| ç®€æ´ç•Œé¢   | 0.05-0.08     | 0.8-0.9       |
| é«˜ç²¾åº¦éœ€æ±‚  | 0.08-0.1      | 0.7           |