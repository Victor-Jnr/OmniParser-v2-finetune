#!/usr/bin/env python3
"""
Florence2 LoRA K-Foldäº¤å‰éªŒè¯å¾®è°ƒè®­ç»ƒå™¨

åŸºäºæ”¹è¿›çš„ finetune_omniparser_lora.pyï¼Œå®ç°K-Foldäº¤å‰éªŒè¯è®­ç»ƒ
é€šè¿‡å¤šæ¬¡è®­ç»ƒè·å¾—æ›´ç¨³å®šå’Œæ³›åŒ–èƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹
"""

import os
import json
import shutil
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoProcessor, AutoModelForCausalLM, 
    get_scheduler, Trainer, TrainingArguments
)
from torch.optim import AdamW
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from PIL import Image, ImageDraw
import numpy as np
from typing import List, Dict, Tuple, Optional
from ultralytics import YOLO
from tqdm import tqdm
import yaml
import cv2
import random
import time
import argparse
from pathlib import Path
from collections import defaultdict, Counter

# å¯¼å…¥åŸå§‹è®­ç»ƒå™¨çš„ç»„ä»¶
from finetune_omniparser_lora import (
    Florence2LoRAModelTrainer, 
    Florence2LoRADataset, 
    collate_fn_florence_lora,
    prepare_training_data
)

class Florence2LoRAKFoldTrainer:
    """
    Florence2 LoRA K-Foldäº¤å‰éªŒè¯è®­ç»ƒå™¨
    
    ç‰¹æ€§ï¼š
    - åŸºäºå›¾åƒçš„K-Foldåˆ†å‰²ï¼Œé¿å…æ•°æ®æ³„éœ²
    - å¤šè½®äº¤å‰éªŒè¯è®­ç»ƒï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›
    - è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹
    - æ”¯æŒæ¨¡å‹èåˆ
    """
    
    def __init__(self, base_model_path: str = "weights/icon_caption_florence", 
                 k_folds: int = 5, use_bfloat16: bool = False):
        self.base_model_path = base_model_path
        self.k_folds = k_folds
        self.use_bfloat16 = use_bfloat16
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # å­˜å‚¨æ¯ä¸ªfoldçš„ç»“æœ
        self.fold_results = []
        self.best_fold = None
        self.best_val_loss = float('inf')
        
        print(f"=== Florence2 LoRA K-Fold Trainer ===")
        print(f"Base model: {self.base_model_path}")
        print(f"K-Fold splits: {self.k_folds}")
        print(f"Device: {self.device}")
    
    def create_image_based_kfold_splits(self, florence_data: List[Dict], seed: int = 42) -> List[Tuple[List[Dict], List[Dict]]]:
        """
        åˆ›å»ºåŸºäºå›¾åƒçš„K-Foldåˆ†å‰²
        ç¡®ä¿åŒä¸€å›¾åƒçš„æ ·æœ¬ä¸ä¼šåŒæ—¶å‡ºç°åœ¨è®­ç»ƒå’ŒéªŒè¯é›†ä¸­
        """
        print(f"\n=== åˆ›å»º {self.k_folds}-Fold æ•°æ®åˆ†å‰² ===")
        random.seed(seed)
        
        # æŒ‰å›¾åƒåˆ†ç»„
        image_groups = defaultdict(list)
        for item in florence_data:
            image_groups[item['image_path']].append(item)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_images = len(image_groups)
        total_samples = len(florence_data)
        avg_samples_per_image = total_samples / total_images
        
        print(f"æ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»å›¾åƒæ•°: {total_images}")
        print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  å¹³å‡æ¯å›¾åƒæ ·æœ¬æ•°: {avg_samples_per_image:.1f}")
        
        # éªŒè¯Kå€¼æ˜¯å¦åˆç†
        if total_images < self.k_folds:
            print(f"âš ï¸  è­¦å‘Š: å›¾åƒæ•°é‡({total_images})å°äºKå€¼({self.k_folds})")
            self.k_folds = max(2, total_images // 2)
            print(f"è‡ªåŠ¨è°ƒæ•´Kå€¼ä¸º: {self.k_folds}")
        
        # éšæœºæ‰“ä¹±å›¾åƒåˆ—è¡¨
        images = list(image_groups.keys())
        random.shuffle(images)
        
        # åˆ›å»ºKä¸ªfold
        fold_size = len(images) // self.k_folds
        remainder = len(images) % self.k_folds
        
        splits = []
        start_idx = 0
        
        for fold in range(self.k_folds):
            # è®¡ç®—å½“å‰foldçš„å¤§å°ï¼ˆå¤„ç†ä½™æ•°ï¼‰
            current_fold_size = fold_size + (1 if fold < remainder else 0)
            end_idx = start_idx + current_fold_size
            
            # éªŒè¯é›†å›¾åƒ
            val_images = images[start_idx:end_idx]
            # è®­ç»ƒé›†å›¾åƒ
            train_images = images[:start_idx] + images[end_idx:]
            
            # ç”Ÿæˆè®­ç»ƒå’ŒéªŒè¯æ•°æ®
            train_data = []
            val_data = []
            
            for img in train_images:
                train_data.extend(image_groups[img])
            
            for img in val_images:
                val_data.extend(image_groups[img])
            
            splits.append((train_data, val_data))
            
            print(f"Fold {fold + 1}:")
            print(f"  è®­ç»ƒ: {len(train_data)} æ ·æœ¬ ({len(train_images)} å›¾åƒ)")
            print(f"  éªŒè¯: {len(val_data)} æ ·æœ¬ ({len(val_images)} å›¾åƒ)")
            
            start_idx = end_idx
        
        return splits
    
    def train_single_fold(self, fold_idx: int, train_data: List[Dict], val_data: List[Dict],
                         epochs: int = 15, batch_size: int = 16, lr: float = 5e-5,
                         lora_r: int = 16, lora_alpha: int = 32, lora_dropout: float = 0.1):
        """è®­ç»ƒå•ä¸ªfold"""
        print(f"\n=== è®­ç»ƒ Fold {fold_idx + 1}/{self.k_folds} ===")
        
        # åˆ›å»ºç‹¬ç«‹çš„è®­ç»ƒå™¨å®ä¾‹
        trainer = Florence2LoRAModelTrainer(
            base_model_path=self.base_model_path,
            use_bfloat16=self.use_bfloat16
        )
        
        # è®¾ç½®æ¨¡å‹
        trainer.setup_model_and_processor(
            lora_r=lora_r,
            lora_alpha=lora_alpha, 
            lora_dropout=lora_dropout
        )
        
        # å‡†å¤‡æ•°æ®åŠ è½½å™¨
        train_dataset = Florence2LoRADataset(train_data, trainer.processor)
        val_dataset = Florence2LoRADataset(val_data, trainer.processor)
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=collate_fn_florence_lora,
            num_workers=0
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=collate_fn_florence_lora,
            num_workers=0
        )
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer, lr_scheduler, trainable_params = trainer._create_optimizer_and_scheduler(
            lr, 0.1, train_loader, epochs
        )
        
        # è®­ç»ƒå¾ªç¯
        best_val_loss = float('inf')
        fold_history = []
        patience = 3
        patience_counter = 0
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            trainer.model.train()
            train_loss = 0
            train_batches = 0
            
            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Fold {fold_idx+1} Epoch {epoch+1}")):
                try:
                    questions, answers, images = batch
                    
                    if not images:
                        continue
                    
                    # å¤„ç†å›¾åƒ
                    processed_images = []
                    for img in images:
                        if hasattr(img, 'convert'):
                            processed_images.append(img.convert('RGB'))
                        elif isinstance(img, np.ndarray):
                            processed_images.append(Image.fromarray(img).convert('RGB'))
                        elif isinstance(img, str) and os.path.exists(img):
                            processed_images.append(Image.open(img).convert('RGB'))
                        else:
                            continue
                    
                    if not processed_images:
                        continue
                    
                    # å¤„ç†è¾“å…¥
                    inputs = trainer.processor(
                        text=questions[:len(processed_images)],
                        images=processed_images,
                        return_tensors="pt",
                        padding=True,
                        do_resize=False
                    )
                    
                    inputs = {k: v.to(trainer.device) for k, v in inputs.items()}
                    
                    # å¤„ç†æ ‡ç­¾
                    valid_answers = answers[:len(processed_images)]
                    labels = trainer.processor.tokenizer(
                        text=valid_answers,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=50,
                        return_token_type_ids=False
                    ).input_ids.to(trainer.device)
                    
                    # å‰å‘ä¼ æ’­
                    outputs = trainer.model(
                        input_ids=inputs["input_ids"],
                        pixel_values=inputs["pixel_values"],
                        labels=labels
                    )
                    
                    if not hasattr(outputs, 'loss') or outputs.loss is None:
                        continue
                    
                    loss = outputs.loss
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)
                    
                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad()
                    
                    train_loss += loss.item()
                    train_batches += 1
                    
                except Exception as e:
                    print(f"Error in batch {batch_idx}: {e}")
                    continue
            
            avg_train_loss = train_loss / train_batches if train_batches > 0 else 0
            
            # éªŒè¯
            val_loss = trainer.validate(val_loader)
            
            print(f"Fold {fold_idx+1} Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}")
            
            fold_history.append({
                'epoch': epoch + 1,
                'train_loss': avg_train_loss,
                'val_loss': val_loss
            })
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                fold_save_path = f"weights/icon_caption_florence_lora_fold_{fold_idx+1}"
                trainer.save_lora_model(fold_save_path)
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"Early stopping at epoch {epoch + 1}")
                    break
        
        # è®°å½•foldç»“æœ
        fold_result = {
            'fold_idx': fold_idx,
            'best_val_loss': best_val_loss,
            'train_samples': len(train_data),
            'val_samples': len(val_data),
            'history': fold_history,
            'model_path': fold_save_path
        }
        
        self.fold_results.append(fold_result)
        
        # æ›´æ–°å…¨å±€æœ€ä½³æ¨¡å‹
        if best_val_loss < self.best_val_loss:
            self.best_val_loss = best_val_loss
            self.best_fold = fold_idx
        
        print(f"Fold {fold_idx+1} å®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        
        # æ¸…ç†å†…å­˜
        del trainer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return fold_result
    
    def train_kfold(self, florence_data: List[Dict], 
                   epochs: int = 15, batch_size: int = 16, lr: float = 5e-5,
                   lora_r: int = 16, lora_alpha: int = 32, lora_dropout: float = 0.1,
                   seed: int = 42):
        """æ‰§è¡ŒK-Foldäº¤å‰éªŒè¯è®­ç»ƒ"""
        print(f"\n=== å¼€å§‹ {self.k_folds}-Fold äº¤å‰éªŒè¯è®­ç»ƒ ===")
        
        # åˆ›å»ºæ•°æ®åˆ†å‰²
        splits = self.create_image_based_kfold_splits(florence_data, seed)
        
        # è®­ç»ƒæ¯ä¸ªfold
        for fold_idx, (train_data, val_data) in enumerate(splits):
            fold_result = self.train_single_fold(
                fold_idx, train_data, val_data,
                epochs=epochs, batch_size=batch_size, lr=lr,
                lora_r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout
            )
        
        # åˆ†æç»“æœ
        self.analyze_results()
        
        # ä¿å­˜ç»“æœ
        self.save_kfold_results()
    
    def analyze_results(self):
        """åˆ†æK-Foldè®­ç»ƒç»“æœ"""
        print(f"\n=== K-Fold è®­ç»ƒç»“æœåˆ†æ ===")
        
        val_losses = [result['best_val_loss'] for result in self.fold_results]
        
        mean_val_loss = np.mean(val_losses)
        std_val_loss = np.std(val_losses)
        min_val_loss = np.min(val_losses)
        max_val_loss = np.max(val_losses)
        
        print(f"éªŒè¯æŸå¤±ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {mean_val_loss:.4f} Â± {std_val_loss:.4f}")
        print(f"  æœ€å°å€¼: {min_val_loss:.4f} (Fold {np.argmin(val_losses) + 1})")
        print(f"  æœ€å¤§å€¼: {max_val_loss:.4f} (Fold {np.argmax(val_losses) + 1})")
        
        print(f"\nå„Foldè¯¦ç»†ç»“æœ:")
        for i, result in enumerate(self.fold_results):
            print(f"  Fold {i+1}: {result['best_val_loss']:.4f} "
                  f"({result['train_samples']} train, {result['val_samples']} val)")
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: Fold {self.best_fold + 1} (éªŒè¯æŸå¤±: {self.best_val_loss:.4f})")
        
        # æ¨¡å‹ç¨³å®šæ€§è¯„ä¼°
        cv_score = std_val_loss / mean_val_loss if mean_val_loss > 0 else 0
        print(f"æ¨¡å‹ç¨³å®šæ€§ (CV): {cv_score:.4f} {'(ç¨³å®š)' if cv_score < 0.1 else '(ä¸ç¨³å®š)' if cv_score > 0.2 else '(ä¸€èˆ¬)'}")
    
    def save_kfold_results(self):
        """ä¿å­˜K-Foldè®­ç»ƒç»“æœ"""
        results_dir = "weights/kfold_results"
        os.makedirs(results_dir, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        kfold_summary = {
            'k_folds': self.k_folds,
            'best_fold': self.best_fold,
            'best_val_loss': self.best_val_loss,
            'fold_results': self.fold_results,
            'summary_stats': {
                'mean_val_loss': np.mean([r['best_val_loss'] for r in self.fold_results]),
                'std_val_loss': np.std([r['best_val_loss'] for r in self.fold_results]),
                'min_val_loss': np.min([r['best_val_loss'] for r in self.fold_results]),
                'max_val_loss': np.max([r['best_val_loss'] for r in self.fold_results])
            },
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        results_file = os.path.join(results_dir, f"kfold_results_{self.k_folds}fold.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(kfold_summary, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ K-Foldç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # å¤åˆ¶æœ€ä½³æ¨¡å‹
        best_model_path = self.fold_results[self.best_fold]['model_path']
        best_model_copy = "weights/icon_caption_florence_lora_kfold_best"
        
        if os.path.exists(best_model_path):
            if os.path.exists(best_model_copy):
                shutil.rmtree(best_model_copy)
            shutil.copytree(best_model_path, best_model_copy)
            print(f"âœ“ æœ€ä½³æ¨¡å‹å·²å¤åˆ¶åˆ°: {best_model_copy}")
        
        return results_file
    
    def ensemble_predictions(self, test_data: List[Dict], top_k: int = 3):
        """
        æ¨¡å‹é›†æˆé¢„æµ‹ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
        ä½¿ç”¨è¡¨ç°æœ€å¥½çš„Kä¸ªæ¨¡å‹è¿›è¡Œé›†æˆé¢„æµ‹
        """
        print(f"\n=== æ¨¡å‹é›†æˆé¢„æµ‹ (Top-{top_k}) ===")
        
        # é€‰æ‹©è¡¨ç°æœ€å¥½çš„Kä¸ªæ¨¡å‹
        sorted_results = sorted(self.fold_results, key=lambda x: x['best_val_loss'])
        top_models = sorted_results[:top_k]
        
        print(f"é€‰æ‹©çš„æ¨¡å‹:")
        for i, result in enumerate(top_models):
            print(f"  {i+1}. Fold {result['fold_idx']+1}: {result['best_val_loss']:.4f}")
        
        # è¿™é‡Œå¯ä»¥å®ç°é›†æˆé¢„æµ‹é€»è¾‘
        # ç”±äºéœ€è¦åŠ è½½å¤šä¸ªæ¨¡å‹ï¼Œå®é™…å®ç°ä¼šæ¯”è¾ƒå¤æ‚
        print("é›†æˆé¢„æµ‹åŠŸèƒ½å¾…å®ç°...")

def calculate_optimal_k(data_size: int, unique_images: int) -> int:
    """æ ¹æ®æ•°æ®é‡è®¡ç®—æœ€ä¼˜Kå€¼"""
    if unique_images >= 80:
        return 5
    elif unique_images >= 50:
        return 4
    elif unique_images >= 30:
        return 3
    else:
        return 2

def main():
    """K-Foldè®­ç»ƒä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Florence2 LoRA K-Fold Cross-Validation Training")
    
    parser.add_argument("--data", type=str, default="training_data/florence_format/florence_data.json")
    parser.add_argument("--model_path", type=str, default="weights/icon_caption_florence")
    parser.add_argument("--k_folds", type=int, default=0, help="K-Fold splits (0 for auto)")
    parser.add_argument("--epochs", type=int, default=15)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--lr", type=float, default=5e-5)
    parser.add_argument("--lora_r", type=int, default=16)
    parser.add_argument("--lora_alpha", type=int, default=32)
    parser.add_argument("--lora_dropout", type=float, default=0.1)
    parser.add_argument("--seed", type=int, default=42)
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import peft
        print(f"âœ“ PEFT library version: {peft.__version__}")
    except ImportError:
        print("âœ— PEFT library not found. Please install with: pip install peft")
        return
    
    # åŠ è½½æ•°æ®
    print("=== åŠ è½½è®­ç»ƒæ•°æ® ===")
    florence_data = prepare_training_data(args.data)
    
    if not florence_data:
        print("No training data available!")
        return
    
    # è®¡ç®—æœ€ä¼˜Kå€¼
    image_counts = Counter([item['image_path'] for item in florence_data])
    unique_images = len(image_counts)
    
    if args.k_folds == 0:
        k_folds = calculate_optimal_k(len(florence_data), unique_images)
        print(f"è‡ªåŠ¨è®¡ç®—æœ€ä¼˜Kå€¼: {k_folds}")
    else:
        k_folds = args.k_folds
    
    # åˆ›å»ºK-Foldè®­ç»ƒå™¨
    trainer = Florence2LoRAKFoldTrainer(
        base_model_path=args.model_path,
        k_folds=k_folds,
        use_bfloat16=False
    )
    
    # å¼€å§‹K-Foldè®­ç»ƒ
    try:
        trainer.train_kfold(
            florence_data=florence_data,
            epochs=20,
            batch_size=16,
            lr=5e-5,
            lora_r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            seed=42
        )
        
        print("\nğŸ‰ K-Foldäº¤å‰éªŒè¯è®­ç»ƒå®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ K-Foldè®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()